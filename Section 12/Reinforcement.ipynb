{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build your neural network model \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    79/50000: episode: 1, duration: 0.976s, episode steps: 79, steps per second: 81, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.426841, mean_absolute_error: 0.494516, mean_q: 0.053988\n",
      "   113/50000: episode: 2, duration: 0.107s, episode steps: 34, steps per second: 318, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.350344, mean_absolute_error: 0.444455, mean_q: 0.192739\n",
      "   163/50000: episode: 3, duration: 0.170s, episode steps: 50, steps per second: 295, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.315713, mean_absolute_error: 0.466836, mean_q: 0.319681\n",
      "   197/50000: episode: 4, duration: 0.122s, episode steps: 34, steps per second: 279, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.278004, mean_absolute_error: 0.502843, mean_q: 0.467669\n",
      "   264/50000: episode: 5, duration: 0.247s, episode steps: 67, steps per second: 271, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.007 [-0.394, 0.770], loss: 0.233190, mean_absolute_error: 0.566349, mean_q: 0.682860\n",
      "   299/50000: episode: 6, duration: 0.118s, episode steps: 35, steps per second: 297, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.095 [-0.204, 0.813], loss: 0.178604, mean_absolute_error: 0.648513, mean_q: 0.954941\n",
      "   336/50000: episode: 7, duration: 0.170s, episode steps: 37, steps per second: 217, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.076 [-0.348, 0.844], loss: 0.142041, mean_absolute_error: 0.732026, mean_q: 1.174971\n",
      "   370/50000: episode: 8, duration: 0.219s, episode steps: 34, steps per second: 155, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.405, 0.902], loss: 0.107080, mean_absolute_error: 0.811291, mean_q: 1.426657\n",
      "   392/50000: episode: 9, duration: 0.100s, episode steps: 22, steps per second: 220, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.353, 0.912], loss: 0.097042, mean_absolute_error: 0.886181, mean_q: 1.628816\n",
      "   412/50000: episode: 10, duration: 0.077s, episode steps: 20, steps per second: 261, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.413, 0.874], loss: 0.093331, mean_absolute_error: 0.972688, mean_q: 1.800804\n",
      "   433/50000: episode: 11, duration: 0.091s, episode steps: 21, steps per second: 232, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.081 [-0.433, 0.946], loss: 0.077446, mean_absolute_error: 1.022093, mean_q: 1.971752\n",
      "   453/50000: episode: 12, duration: 0.075s, episode steps: 20, steps per second: 267, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.091 [-0.546, 1.090], loss: 0.084138, mean_absolute_error: 1.115697, mean_q: 2.146566\n",
      "   480/50000: episode: 13, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.084 [-0.372, 1.090], loss: 0.094632, mean_absolute_error: 1.225402, mean_q: 2.344754\n",
      "   502/50000: episode: 14, duration: 0.090s, episode steps: 22, steps per second: 245, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.078 [-0.582, 1.176], loss: 0.124918, mean_absolute_error: 1.330129, mean_q: 2.554960\n",
      "   515/50000: episode: 15, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.093 [-0.795, 1.372], loss: 0.134287, mean_absolute_error: 1.400611, mean_q: 2.699338\n",
      "   527/50000: episode: 16, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.109 [-0.590, 1.193], loss: 0.117695, mean_absolute_error: 1.444211, mean_q: 2.813307\n",
      "   547/50000: episode: 17, duration: 0.080s, episode steps: 20, steps per second: 250, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.061 [-0.582, 1.187], loss: 0.133651, mean_absolute_error: 1.520836, mean_q: 2.965058\n",
      "   563/50000: episode: 18, duration: 0.057s, episode steps: 16, steps per second: 281, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-0.992, 1.604], loss: 0.135285, mean_absolute_error: 1.580697, mean_q: 3.113644\n",
      "   578/50000: episode: 19, duration: 0.057s, episode steps: 15, steps per second: 262, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.118 [-0.758, 1.266], loss: 0.194028, mean_absolute_error: 1.681694, mean_q: 3.263210\n",
      "   591/50000: episode: 20, duration: 0.055s, episode steps: 13, steps per second: 238, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.108 [-0.787, 1.447], loss: 0.208697, mean_absolute_error: 1.743569, mean_q: 3.377490\n",
      "   602/50000: episode: 21, duration: 0.038s, episode steps: 11, steps per second: 293, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.140 [-0.770, 1.469], loss: 0.133439, mean_absolute_error: 1.782336, mean_q: 3.522725\n",
      "   613/50000: episode: 22, duration: 0.038s, episode steps: 11, steps per second: 287, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.112 [-0.989, 1.513], loss: 0.207212, mean_absolute_error: 1.857713, mean_q: 3.638355\n",
      "   625/50000: episode: 23, duration: 0.043s, episode steps: 12, steps per second: 281, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-0.757, 1.513], loss: 0.275407, mean_absolute_error: 1.936682, mean_q: 3.808287\n",
      "   635/50000: episode: 24, duration: 0.043s, episode steps: 10, steps per second: 234, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-0.968, 1.629], loss: 0.255023, mean_absolute_error: 1.978261, mean_q: 3.832887\n",
      "   647/50000: episode: 25, duration: 0.052s, episode steps: 12, steps per second: 229, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-0.996, 1.605], loss: 0.334438, mean_absolute_error: 2.031629, mean_q: 3.936363\n",
      "   656/50000: episode: 26, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-0.969, 1.736], loss: 0.256843, mean_absolute_error: 2.074016, mean_q: 4.057959\n",
      "   671/50000: episode: 27, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.067 [-0.995, 1.666], loss: 0.345962, mean_absolute_error: 2.147228, mean_q: 4.185690\n",
      "   683/50000: episode: 28, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-1.033, 1.737], loss: 0.342707, mean_absolute_error: 2.184767, mean_q: 4.251537\n",
      "   694/50000: episode: 29, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.143 [-1.128, 1.841], loss: 0.401247, mean_absolute_error: 2.271779, mean_q: 4.382195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   705/50000: episode: 30, duration: 0.045s, episode steps: 11, steps per second: 247, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-0.942, 1.734], loss: 0.373739, mean_absolute_error: 2.326306, mean_q: 4.484053\n",
      "   720/50000: episode: 31, duration: 0.062s, episode steps: 15, steps per second: 241, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.073 [-1.015, 1.653], loss: 0.388497, mean_absolute_error: 2.357814, mean_q: 4.605469\n",
      "   730/50000: episode: 32, duration: 0.034s, episode steps: 10, steps per second: 294, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.108 [-1.028, 1.700], loss: 0.352324, mean_absolute_error: 2.438974, mean_q: 4.741551\n",
      "   740/50000: episode: 33, duration: 0.044s, episode steps: 10, steps per second: 230, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.144 [-1.348, 2.185], loss: 0.356990, mean_absolute_error: 2.470826, mean_q: 4.886415\n",
      "   749/50000: episode: 34, duration: 0.027s, episode steps: 9, steps per second: 330, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.516, 2.495], loss: 0.481809, mean_absolute_error: 2.544262, mean_q: 4.983852\n",
      "   758/50000: episode: 35, duration: 0.034s, episode steps: 9, steps per second: 264, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.197, 1.894], loss: 0.366598, mean_absolute_error: 2.538419, mean_q: 4.973547\n",
      "   768/50000: episode: 36, duration: 0.041s, episode steps: 10, steps per second: 246, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.114 [-1.391, 2.145], loss: 0.680696, mean_absolute_error: 2.685109, mean_q: 5.107930\n",
      "   778/50000: episode: 37, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.547, 2.430], loss: 0.446691, mean_absolute_error: 2.655722, mean_q: 5.104411\n",
      "   786/50000: episode: 38, duration: 0.032s, episode steps: 8, steps per second: 251, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.547, 2.531], loss: 0.818131, mean_absolute_error: 2.793761, mean_q: 5.264208\n",
      "   798/50000: episode: 39, duration: 0.043s, episode steps: 12, steps per second: 276, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.102 [-1.956, 3.002], loss: 0.598327, mean_absolute_error: 2.738996, mean_q: 5.274708\n",
      "   807/50000: episode: 40, duration: 0.038s, episode steps: 9, steps per second: 236, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.577, 2.472], loss: 0.565489, mean_absolute_error: 2.814816, mean_q: 5.378797\n",
      "   815/50000: episode: 41, duration: 0.032s, episode steps: 8, steps per second: 251, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.175, 2.000], loss: 0.655344, mean_absolute_error: 2.850535, mean_q: 5.481163\n",
      "   825/50000: episode: 42, duration: 0.046s, episode steps: 10, steps per second: 218, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.108 [-1.030, 1.642], loss: 0.664858, mean_absolute_error: 2.899332, mean_q: 5.501908\n",
      "   834/50000: episode: 43, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.160 [-0.948, 1.776], loss: 0.865325, mean_absolute_error: 2.987808, mean_q: 5.588766\n",
      "   845/50000: episode: 44, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.133, 1.803], loss: 1.004184, mean_absolute_error: 3.049666, mean_q: 5.606004\n",
      "   855/50000: episode: 45, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.141 [-0.943, 1.674], loss: 0.954434, mean_absolute_error: 3.057024, mean_q: 5.597602\n",
      "   865/50000: episode: 46, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.152 [-1.533, 2.591], loss: 0.688525, mean_absolute_error: 3.050652, mean_q: 5.702453\n",
      "   874/50000: episode: 47, duration: 0.034s, episode steps: 9, steps per second: 266, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.525, 2.446], loss: 0.956526, mean_absolute_error: 3.097498, mean_q: 5.870908\n",
      "   883/50000: episode: 48, duration: 0.033s, episode steps: 9, steps per second: 274, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.794, 2.794], loss: 0.436676, mean_absolute_error: 3.023600, mean_q: 5.905766\n",
      "   892/50000: episode: 49, duration: 0.031s, episode steps: 9, steps per second: 286, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.132 [-1.379, 2.220], loss: 0.820376, mean_absolute_error: 3.165332, mean_q: 6.096688\n",
      "   902/50000: episode: 50, duration: 0.030s, episode steps: 10, steps per second: 336, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.105 [-1.390, 2.116], loss: 0.443020, mean_absolute_error: 3.126770, mean_q: 6.150025\n",
      "   912/50000: episode: 51, duration: 0.031s, episode steps: 10, steps per second: 324, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.336, 2.180], loss: 0.894414, mean_absolute_error: 3.252583, mean_q: 6.205224\n",
      "   923/50000: episode: 52, duration: 0.040s, episode steps: 11, steps per second: 272, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.404, 2.186], loss: 1.172383, mean_absolute_error: 3.355205, mean_q: 6.217674\n",
      "   933/50000: episode: 53, duration: 0.035s, episode steps: 10, steps per second: 288, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.122 [-1.605, 2.434], loss: 1.117627, mean_absolute_error: 3.365793, mean_q: 6.173723\n",
      "   943/50000: episode: 54, duration: 0.031s, episode steps: 10, steps per second: 322, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.375, 2.178], loss: 1.066483, mean_absolute_error: 3.369522, mean_q: 6.222955\n",
      "   952/50000: episode: 55, duration: 0.027s, episode steps: 9, steps per second: 332, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.742, 2.813], loss: 0.921670, mean_absolute_error: 3.387192, mean_q: 6.302655\n",
      "   965/50000: episode: 56, duration: 0.049s, episode steps: 13, steps per second: 265, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.087 [-1.590, 2.341], loss: 1.116034, mean_absolute_error: 3.458797, mean_q: 6.384821\n",
      "   974/50000: episode: 57, duration: 0.030s, episode steps: 9, steps per second: 300, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.122 [-1.189, 1.884], loss: 1.069499, mean_absolute_error: 3.498235, mean_q: 6.477259\n",
      "   987/50000: episode: 58, duration: 0.040s, episode steps: 13, steps per second: 326, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.106 [-1.148, 1.926], loss: 0.880280, mean_absolute_error: 3.485182, mean_q: 6.497760\n",
      "   996/50000: episode: 59, duration: 0.030s, episode steps: 9, steps per second: 301, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.125 [-1.023, 1.756], loss: 1.019249, mean_absolute_error: 3.531693, mean_q: 6.581190\n",
      "  1009/50000: episode: 60, duration: 0.052s, episode steps: 13, steps per second: 249, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.101 [-1.018, 1.490], loss: 0.858686, mean_absolute_error: 3.560874, mean_q: 6.642178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1019/50000: episode: 61, duration: 0.035s, episode steps: 10, steps per second: 282, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.100 [-0.994, 1.596], loss: 0.733594, mean_absolute_error: 3.551845, mean_q: 6.753257\n",
      "  1030/50000: episode: 62, duration: 0.043s, episode steps: 11, steps per second: 254, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.111 [-1.171, 1.905], loss: 1.206318, mean_absolute_error: 3.663323, mean_q: 6.831096\n",
      "  1043/50000: episode: 63, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.094 [-1.323, 1.963], loss: 1.128772, mean_absolute_error: 3.673794, mean_q: 6.831375\n",
      "  1055/50000: episode: 64, duration: 0.058s, episode steps: 12, steps per second: 209, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-1.131, 1.807], loss: 0.957678, mean_absolute_error: 3.683675, mean_q: 6.823706\n",
      "  1068/50000: episode: 65, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-0.999, 1.699], loss: 0.931409, mean_absolute_error: 3.732083, mean_q: 6.937922\n",
      "  1079/50000: episode: 66, duration: 0.047s, episode steps: 11, steps per second: 233, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.119 [-1.158, 1.888], loss: 1.081562, mean_absolute_error: 3.775928, mean_q: 7.083408\n",
      "  1090/50000: episode: 67, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.370, 2.033], loss: 1.329426, mean_absolute_error: 3.854251, mean_q: 7.025805\n",
      "  1100/50000: episode: 68, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.154, 1.896], loss: 1.071012, mean_absolute_error: 3.803601, mean_q: 6.960317\n",
      "  1111/50000: episode: 69, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.101 [-1.158, 1.747], loss: 1.418364, mean_absolute_error: 3.921117, mean_q: 7.026530\n",
      "  1123/50000: episode: 70, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.126 [-0.968, 1.659], loss: 1.127602, mean_absolute_error: 3.860914, mean_q: 7.051068\n",
      "  1135/50000: episode: 71, duration: 0.055s, episode steps: 12, steps per second: 219, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.419, 2.279], loss: 1.169367, mean_absolute_error: 3.877110, mean_q: 7.139838\n",
      "  1145/50000: episode: 72, duration: 0.043s, episode steps: 10, steps per second: 233, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.540, 2.638], loss: 1.112830, mean_absolute_error: 3.912603, mean_q: 7.348609\n",
      "  1155/50000: episode: 73, duration: 0.044s, episode steps: 10, steps per second: 227, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-2.000, 3.117], loss: 1.147692, mean_absolute_error: 3.928633, mean_q: 7.321461\n",
      "  1163/50000: episode: 74, duration: 0.064s, episode steps: 8, steps per second: 124, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.336, 2.211], loss: 0.924921, mean_absolute_error: 3.923172, mean_q: 7.316390\n",
      "  1174/50000: episode: 75, duration: 0.045s, episode steps: 11, steps per second: 246, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.103 [-1.154, 1.871], loss: 1.330565, mean_absolute_error: 3.990520, mean_q: 7.317273\n",
      "  1184/50000: episode: 76, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.151 [-1.132, 1.886], loss: 0.897479, mean_absolute_error: 3.979735, mean_q: 7.384197\n",
      "  1196/50000: episode: 77, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-1.149, 1.800], loss: 1.053351, mean_absolute_error: 3.992659, mean_q: 7.388861\n",
      "  1205/50000: episode: 78, duration: 0.040s, episode steps: 9, steps per second: 228, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-0.943, 1.705], loss: 1.046251, mean_absolute_error: 4.031506, mean_q: 7.452716\n",
      "  1216/50000: episode: 79, duration: 0.040s, episode steps: 11, steps per second: 275, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-0.951, 1.728], loss: 0.956921, mean_absolute_error: 4.032264, mean_q: 7.501950\n",
      "  1228/50000: episode: 80, duration: 0.041s, episode steps: 12, steps per second: 294, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.109 [-0.936, 1.524], loss: 0.974451, mean_absolute_error: 4.070834, mean_q: 7.532413\n",
      "  1245/50000: episode: 81, duration: 0.055s, episode steps: 17, steps per second: 307, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.092 [-0.588, 1.267], loss: 1.238592, mean_absolute_error: 4.132847, mean_q: 7.586900\n",
      "  1258/50000: episode: 82, duration: 0.053s, episode steps: 13, steps per second: 243, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-0.933, 1.450], loss: 0.890752, mean_absolute_error: 4.106700, mean_q: 7.645478\n",
      "  1271/50000: episode: 83, duration: 0.052s, episode steps: 13, steps per second: 248, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-1.002, 1.564], loss: 1.257251, mean_absolute_error: 4.204941, mean_q: 7.729144\n",
      "  1283/50000: episode: 84, duration: 0.041s, episode steps: 12, steps per second: 295, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.107 [-0.744, 1.329], loss: 0.921631, mean_absolute_error: 4.178028, mean_q: 7.753448\n",
      "  1298/50000: episode: 85, duration: 0.051s, episode steps: 15, steps per second: 297, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.100 [-0.601, 1.014], loss: 1.279368, mean_absolute_error: 4.204939, mean_q: 7.618354\n",
      "  1315/50000: episode: 86, duration: 0.057s, episode steps: 17, steps per second: 297, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.090 [-0.785, 1.503], loss: 1.276283, mean_absolute_error: 4.272384, mean_q: 7.749558\n",
      "  1326/50000: episode: 87, duration: 0.050s, episode steps: 11, steps per second: 221, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.128, 1.860], loss: 1.049049, mean_absolute_error: 4.193165, mean_q: 7.650271\n",
      "  1349/50000: episode: 88, duration: 0.085s, episode steps: 23, steps per second: 271, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.072 [-0.813, 1.132], loss: 1.118114, mean_absolute_error: 4.265186, mean_q: 7.828335\n",
      "  1409/50000: episode: 89, duration: 0.313s, episode steps: 60, steps per second: 192, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.076 [-0.415, 0.928], loss: 1.111589, mean_absolute_error: 4.334043, mean_q: 7.996649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1471/50000: episode: 90, duration: 0.224s, episode steps: 62, steps per second: 277, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.078 [-0.364, 0.862], loss: 0.982383, mean_absolute_error: 4.447815, mean_q: 8.290794\n",
      "  1511/50000: episode: 91, duration: 0.134s, episode steps: 40, steps per second: 297, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.093 [-0.795, 0.235], loss: 1.001549, mean_absolute_error: 4.586510, mean_q: 8.615925\n",
      "  1535/50000: episode: 92, duration: 0.083s, episode steps: 24, steps per second: 288, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-0.975, 0.367], loss: 1.145196, mean_absolute_error: 4.709943, mean_q: 8.834752\n",
      "  1561/50000: episode: 93, duration: 0.085s, episode steps: 26, steps per second: 304, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.971, 0.222], loss: 1.310311, mean_absolute_error: 4.753067, mean_q: 8.814790\n",
      "  1579/50000: episode: 94, duration: 0.063s, episode steps: 18, steps per second: 286, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.095 [-1.289, 0.581], loss: 0.877742, mean_absolute_error: 4.800882, mean_q: 9.075415\n",
      "  1611/50000: episode: 95, duration: 0.109s, episode steps: 32, steps per second: 294, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.020 [-1.581, 0.837], loss: 1.147501, mean_absolute_error: 4.938478, mean_q: 9.399824\n",
      "  1627/50000: episode: 96, duration: 0.059s, episode steps: 16, steps per second: 270, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.085 [-2.117, 1.231], loss: 0.935563, mean_absolute_error: 4.971739, mean_q: 9.465118\n",
      "  1635/50000: episode: 97, duration: 0.032s, episode steps: 8, steps per second: 253, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.509, 1.552], loss: 1.740821, mean_absolute_error: 5.076776, mean_q: 9.488886\n",
      "  1652/50000: episode: 98, duration: 0.055s, episode steps: 17, steps per second: 311, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.078 [-2.263, 1.403], loss: 1.446347, mean_absolute_error: 5.110861, mean_q: 9.600409\n",
      "  1666/50000: episode: 99, duration: 0.053s, episode steps: 14, steps per second: 266, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.082 [-2.951, 1.941], loss: 1.625808, mean_absolute_error: 5.204641, mean_q: 9.730166\n",
      "  1675/50000: episode: 100, duration: 0.039s, episode steps: 9, steps per second: 231, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.182 [-2.887, 1.722], loss: 2.251393, mean_absolute_error: 5.213973, mean_q: 9.670476\n",
      "  1685/50000: episode: 101, duration: 0.041s, episode steps: 10, steps per second: 244, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.154 [-2.618, 1.577], loss: 3.027449, mean_absolute_error: 5.298178, mean_q: 9.684734\n",
      "  1694/50000: episode: 102, duration: 0.033s, episode steps: 9, steps per second: 273, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.339, 1.364], loss: 1.050254, mean_absolute_error: 5.204730, mean_q: 9.846078\n",
      "  1705/50000: episode: 103, duration: 0.041s, episode steps: 11, steps per second: 270, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.122 [-2.377, 1.386], loss: 1.314680, mean_absolute_error: 5.236288, mean_q: 9.835876\n",
      "  1723/50000: episode: 104, duration: 0.064s, episode steps: 18, steps per second: 279, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.081 [-1.171, 0.600], loss: 1.656193, mean_absolute_error: 5.273595, mean_q: 9.893719\n",
      "  1739/50000: episode: 105, duration: 0.059s, episode steps: 16, steps per second: 272, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.111 [-1.130, 0.571], loss: 2.356439, mean_absolute_error: 5.450808, mean_q: 10.050570\n",
      "  1765/50000: episode: 106, duration: 0.086s, episode steps: 26, steps per second: 302, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.973, 0.218], loss: 1.637778, mean_absolute_error: 5.387779, mean_q: 10.029907\n",
      "  1785/50000: episode: 107, duration: 0.074s, episode steps: 20, steps per second: 271, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.066 [-1.184, 0.614], loss: 2.191726, mean_absolute_error: 5.491108, mean_q: 10.105018\n",
      "  1804/50000: episode: 108, duration: 0.063s, episode steps: 19, steps per second: 304, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.071 [-1.079, 0.456], loss: 1.765186, mean_absolute_error: 5.495885, mean_q: 10.141372\n",
      "  1843/50000: episode: 109, duration: 0.132s, episode steps: 39, steps per second: 296, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.027 [-0.930, 0.615], loss: 2.299172, mean_absolute_error: 5.597058, mean_q: 10.325066\n",
      "  1871/50000: episode: 110, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.073 [-1.109, 0.591], loss: 1.586912, mean_absolute_error: 5.578934, mean_q: 10.352822\n",
      "  1891/50000: episode: 111, duration: 0.084s, episode steps: 20, steps per second: 239, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.098 [-1.331, 0.594], loss: 1.650527, mean_absolute_error: 5.686244, mean_q: 10.738382\n",
      "  1906/50000: episode: 112, duration: 0.062s, episode steps: 15, steps per second: 243, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.093 [-1.003, 0.606], loss: 1.721565, mean_absolute_error: 5.747226, mean_q: 10.893187\n",
      "  1929/50000: episode: 113, duration: 0.078s, episode steps: 23, steps per second: 294, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.058 [-0.974, 0.433], loss: 1.751948, mean_absolute_error: 5.794477, mean_q: 10.927203\n",
      "  1953/50000: episode: 114, duration: 0.088s, episode steps: 24, steps per second: 272, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.075 [-1.198, 0.566], loss: 2.430808, mean_absolute_error: 5.879405, mean_q: 10.925067\n",
      "  1976/50000: episode: 115, duration: 0.090s, episode steps: 23, steps per second: 255, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.079 [-1.300, 0.568], loss: 2.150232, mean_absolute_error: 5.863600, mean_q: 10.955817\n",
      "  2002/50000: episode: 116, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.057 [-1.070, 0.384], loss: 2.030082, mean_absolute_error: 5.953492, mean_q: 11.217182\n",
      "  2026/50000: episode: 117, duration: 0.105s, episode steps: 24, steps per second: 228, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.018, 0.293], loss: 2.477813, mean_absolute_error: 6.056808, mean_q: 11.238533\n",
      "  2041/50000: episode: 118, duration: 0.055s, episode steps: 15, steps per second: 274, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.108 [-1.133, 0.571], loss: 3.018249, mean_absolute_error: 6.047054, mean_q: 11.056915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2056/50000: episode: 119, duration: 0.058s, episode steps: 15, steps per second: 258, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.108 [-1.025, 0.418], loss: 2.888992, mean_absolute_error: 6.101038, mean_q: 11.159227\n",
      "  2077/50000: episode: 120, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.098 [-1.003, 0.269], loss: 1.982455, mean_absolute_error: 6.088154, mean_q: 11.346805\n",
      "  2105/50000: episode: 121, duration: 0.125s, episode steps: 28, steps per second: 225, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-0.984, 0.188], loss: 2.140847, mean_absolute_error: 6.069653, mean_q: 11.350748\n",
      "  2168/50000: episode: 122, duration: 0.270s, episode steps: 63, steps per second: 233, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.002 [-0.838, 0.395], loss: 1.955502, mean_absolute_error: 6.138496, mean_q: 11.578484\n",
      "  2183/50000: episode: 123, duration: 0.067s, episode steps: 15, steps per second: 222, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.123 [-1.061, 0.555], loss: 2.365685, mean_absolute_error: 6.359918, mean_q: 12.000266\n",
      "  2210/50000: episode: 124, duration: 0.097s, episode steps: 27, steps per second: 280, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.053 [-0.978, 0.585], loss: 2.035376, mean_absolute_error: 6.338410, mean_q: 12.045411\n",
      "  2230/50000: episode: 125, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.084 [-1.103, 0.408], loss: 2.507834, mean_absolute_error: 6.457081, mean_q: 12.208572\n",
      "  2244/50000: episode: 126, duration: 0.059s, episode steps: 14, steps per second: 236, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-0.982, 0.434], loss: 1.851839, mean_absolute_error: 6.423059, mean_q: 12.284014\n",
      "  2267/50000: episode: 127, duration: 0.104s, episode steps: 23, steps per second: 221, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.093 [-1.060, 0.262], loss: 2.329978, mean_absolute_error: 6.474681, mean_q: 12.212880\n",
      "  2352/50000: episode: 128, duration: 0.398s, episode steps: 85, steps per second: 213, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.011 [-0.761, 0.442], loss: 2.453670, mean_absolute_error: 6.588304, mean_q: 12.440728\n",
      "  2388/50000: episode: 129, duration: 0.124s, episode steps: 36, steps per second: 291, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-0.869, 0.378], loss: 2.243417, mean_absolute_error: 6.681980, mean_q: 12.583496\n",
      "  2442/50000: episode: 130, duration: 0.194s, episode steps: 54, steps per second: 279, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.921, 0.370], loss: 2.252192, mean_absolute_error: 6.801758, mean_q: 12.987110\n",
      "  2464/50000: episode: 131, duration: 0.070s, episode steps: 22, steps per second: 316, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.099 [-0.988, 0.568], loss: 2.454493, mean_absolute_error: 6.970018, mean_q: 13.248519\n",
      "  2502/50000: episode: 132, duration: 0.129s, episode steps: 38, steps per second: 295, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-0.941, 0.421], loss: 2.448789, mean_absolute_error: 6.973293, mean_q: 13.198913\n",
      "  2551/50000: episode: 133, duration: 0.170s, episode steps: 49, steps per second: 287, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.054 [-0.914, 0.419], loss: 2.404177, mean_absolute_error: 7.107391, mean_q: 13.589558\n",
      "  2582/50000: episode: 134, duration: 0.106s, episode steps: 31, steps per second: 293, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.085 [-0.783, 0.420], loss: 2.342470, mean_absolute_error: 7.149673, mean_q: 13.568015\n",
      "  2605/50000: episode: 135, duration: 0.080s, episode steps: 23, steps per second: 288, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.104 [-1.092, 0.578], loss: 2.504679, mean_absolute_error: 7.218391, mean_q: 13.685401\n",
      "  2655/50000: episode: 136, duration: 0.179s, episode steps: 50, steps per second: 279, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-0.896, 0.351], loss: 2.851638, mean_absolute_error: 7.277099, mean_q: 13.740632\n",
      "  2686/50000: episode: 137, duration: 0.094s, episode steps: 31, steps per second: 330, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.098 [-0.834, 0.208], loss: 2.281754, mean_absolute_error: 7.333490, mean_q: 13.968467\n",
      "  2715/50000: episode: 138, duration: 0.097s, episode steps: 29, steps per second: 299, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.072 [-0.915, 0.427], loss: 2.339124, mean_absolute_error: 7.338215, mean_q: 14.030214\n",
      "  2762/50000: episode: 139, duration: 0.157s, episode steps: 47, steps per second: 299, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.067 [-0.802, 0.355], loss: 3.203934, mean_absolute_error: 7.531114, mean_q: 14.191430\n",
      "  2791/50000: episode: 140, duration: 0.095s, episode steps: 29, steps per second: 305, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.111 [-0.825, 0.214], loss: 2.223441, mean_absolute_error: 7.422813, mean_q: 14.162230\n",
      "  2831/50000: episode: 141, duration: 0.146s, episode steps: 40, steps per second: 274, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-0.955, 0.204], loss: 2.731684, mean_absolute_error: 7.564818, mean_q: 14.432627\n",
      "  2852/50000: episode: 142, duration: 0.075s, episode steps: 21, steps per second: 280, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.111 [-0.804, 0.204], loss: 2.251803, mean_absolute_error: 7.636594, mean_q: 14.650144\n",
      "  2894/50000: episode: 143, duration: 0.130s, episode steps: 42, steps per second: 323, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.143 [-0.241, 0.765], loss: 2.550674, mean_absolute_error: 7.734611, mean_q: 14.812683\n",
      "  2936/50000: episode: 144, duration: 0.118s, episode steps: 42, steps per second: 356, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.075 [-0.769, 0.375], loss: 3.139769, mean_absolute_error: 7.827812, mean_q: 14.896675\n",
      "  2999/50000: episode: 145, duration: 0.151s, episode steps: 63, steps per second: 418, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.144 [-0.237, 0.805], loss: 3.151201, mean_absolute_error: 7.886524, mean_q: 14.962475\n",
      "  3023/50000: episode: 146, duration: 0.061s, episode steps: 24, steps per second: 393, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-0.859, 0.191], loss: 3.123565, mean_absolute_error: 7.942909, mean_q: 15.118061\n",
      "  3049/50000: episode: 147, duration: 0.073s, episode steps: 26, steps per second: 358, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.105 [-0.780, 0.364], loss: 2.929940, mean_absolute_error: 7.973839, mean_q: 15.237062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3097/50000: episode: 148, duration: 0.118s, episode steps: 48, steps per second: 406, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-0.838, 0.229], loss: 3.670322, mean_absolute_error: 8.102551, mean_q: 15.389377\n",
      "  3118/50000: episode: 149, duration: 0.062s, episode steps: 21, steps per second: 336, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.096 [-0.948, 0.409], loss: 2.651317, mean_absolute_error: 8.122108, mean_q: 15.486122\n",
      "  3150/50000: episode: 150, duration: 0.081s, episode steps: 32, steps per second: 395, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-0.946, 0.523], loss: 2.307862, mean_absolute_error: 8.147066, mean_q: 15.725236\n",
      "  3190/50000: episode: 151, duration: 0.108s, episode steps: 40, steps per second: 371, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.076 [-1.028, 0.229], loss: 3.279995, mean_absolute_error: 8.266832, mean_q: 15.794909\n",
      "  3239/50000: episode: 152, duration: 0.119s, episode steps: 49, steps per second: 412, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.082 [-0.798, 0.310], loss: 3.824989, mean_absolute_error: 8.303355, mean_q: 15.679025\n",
      "  3286/50000: episode: 153, duration: 0.118s, episode steps: 47, steps per second: 397, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.084 [-1.310, 0.313], loss: 3.729844, mean_absolute_error: 8.317268, mean_q: 15.740106\n",
      "  3323/50000: episode: 154, duration: 0.096s, episode steps: 37, steps per second: 384, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.095 [-0.902, 0.180], loss: 3.033073, mean_absolute_error: 8.345987, mean_q: 15.880102\n",
      "  3373/50000: episode: 155, duration: 0.129s, episode steps: 50, steps per second: 389, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.073 [-0.796, 0.401], loss: 3.643504, mean_absolute_error: 8.426214, mean_q: 16.044407\n",
      "  3403/50000: episode: 156, duration: 0.075s, episode steps: 30, steps per second: 400, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-0.915, 0.393], loss: 3.539955, mean_absolute_error: 8.616343, mean_q: 16.390089\n",
      "  3443/50000: episode: 157, duration: 0.099s, episode steps: 40, steps per second: 404, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.111 [-0.233, 0.870], loss: 3.143008, mean_absolute_error: 8.616724, mean_q: 16.518250\n",
      "  3521/50000: episode: 158, duration: 0.201s, episode steps: 78, steps per second: 387, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.084 [-0.382, 0.719], loss: 2.910403, mean_absolute_error: 8.622719, mean_q: 16.567627\n",
      "  3577/50000: episode: 159, duration: 0.134s, episode steps: 56, steps per second: 418, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.116 [-0.314, 0.848], loss: 3.785531, mean_absolute_error: 8.817560, mean_q: 16.769018\n",
      "  3609/50000: episode: 160, duration: 0.079s, episode steps: 32, steps per second: 407, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.095 [-0.853, 0.444], loss: 3.153992, mean_absolute_error: 8.907086, mean_q: 17.141392\n",
      "  3750/50000: episode: 161, duration: 0.348s, episode steps: 141, steps per second: 405, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.001 [-0.975, 0.568], loss: 3.639962, mean_absolute_error: 9.021179, mean_q: 17.272696\n",
      "  3804/50000: episode: 162, duration: 0.139s, episode steps: 54, steps per second: 388, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.135 [-0.252, 0.869], loss: 3.172588, mean_absolute_error: 9.140089, mean_q: 17.566322\n",
      "  3846/50000: episode: 163, duration: 0.119s, episode steps: 42, steps per second: 353, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.096 [-0.705, 0.208], loss: 3.080351, mean_absolute_error: 9.291209, mean_q: 17.944080\n",
      "  3924/50000: episode: 164, duration: 0.191s, episode steps: 78, steps per second: 408, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.120 [-0.511, 0.704], loss: 3.769458, mean_absolute_error: 9.367861, mean_q: 17.975117\n",
      "  3957/50000: episode: 165, duration: 0.091s, episode steps: 33, steps per second: 364, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.090 [-1.062, 0.386], loss: 2.073569, mean_absolute_error: 9.429169, mean_q: 18.301483\n",
      "  4012/50000: episode: 166, duration: 0.138s, episode steps: 55, steps per second: 398, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.059 [-0.773, 0.341], loss: 3.497344, mean_absolute_error: 9.549909, mean_q: 18.406088\n",
      "  4125/50000: episode: 167, duration: 0.281s, episode steps: 113, steps per second: 402, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.093 [-0.497, 1.134], loss: 3.669622, mean_absolute_error: 9.699684, mean_q: 18.696218\n",
      "  4167/50000: episode: 168, duration: 0.111s, episode steps: 42, steps per second: 379, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.083 [-0.814, 0.442], loss: 4.681932, mean_absolute_error: 9.842021, mean_q: 18.797935\n",
      "  4220/50000: episode: 169, duration: 0.140s, episode steps: 53, steps per second: 379, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.101 [-0.951, 0.261], loss: 3.709199, mean_absolute_error: 9.859079, mean_q: 18.949911\n",
      "  4263/50000: episode: 170, duration: 0.110s, episode steps: 43, steps per second: 389, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.080 [-0.765, 0.227], loss: 3.900822, mean_absolute_error: 9.861877, mean_q: 18.935312\n",
      "  4307/50000: episode: 171, duration: 0.116s, episode steps: 44, steps per second: 379, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.103 [-0.748, 0.229], loss: 3.323046, mean_absolute_error: 9.955100, mean_q: 19.275099\n",
      "  4361/50000: episode: 172, duration: 0.136s, episode steps: 54, steps per second: 397, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.069 [-0.837, 0.622], loss: 3.645519, mean_absolute_error: 10.027083, mean_q: 19.397511\n",
      "  4403/50000: episode: 173, duration: 0.111s, episode steps: 42, steps per second: 380, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.107 [-0.853, 0.231], loss: 3.380509, mean_absolute_error: 10.110557, mean_q: 19.562441\n",
      "  4491/50000: episode: 174, duration: 0.219s, episode steps: 88, steps per second: 402, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.097 [-0.398, 0.824], loss: 3.913542, mean_absolute_error: 10.207069, mean_q: 19.687445\n",
      "  4561/50000: episode: 175, duration: 0.179s, episode steps: 70, steps per second: 392, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.111 [-0.683, 0.721], loss: 3.911694, mean_absolute_error: 10.276118, mean_q: 19.766348\n",
      "  4592/50000: episode: 176, duration: 0.085s, episode steps: 31, steps per second: 366, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.126 [-0.807, 0.179], loss: 4.208949, mean_absolute_error: 10.340370, mean_q: 19.877760\n",
      "  4629/50000: episode: 177, duration: 0.095s, episode steps: 37, steps per second: 391, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.086 [-0.906, 0.302], loss: 4.778264, mean_absolute_error: 10.567912, mean_q: 20.215170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4731/50000: episode: 178, duration: 0.252s, episode steps: 102, steps per second: 405, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.273, 0.371], loss: 3.109821, mean_absolute_error: 10.451025, mean_q: 20.260960\n",
      "  4812/50000: episode: 179, duration: 0.204s, episode steps: 81, steps per second: 398, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.025 [-0.764, 0.577], loss: 3.323465, mean_absolute_error: 10.643510, mean_q: 20.684933\n",
      "  4842/50000: episode: 180, duration: 0.080s, episode steps: 30, steps per second: 376, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.115 [-0.732, 0.214], loss: 2.605819, mean_absolute_error: 10.656039, mean_q: 20.783533\n",
      "  4886/50000: episode: 181, duration: 0.112s, episode steps: 44, steps per second: 392, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.082 [-0.728, 0.258], loss: 3.779216, mean_absolute_error: 10.837990, mean_q: 20.990126\n",
      "  4924/50000: episode: 182, duration: 0.104s, episode steps: 38, steps per second: 366, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.116 [-0.727, 0.169], loss: 2.979871, mean_absolute_error: 10.879750, mean_q: 21.189423\n",
      "  4965/50000: episode: 183, duration: 0.108s, episode steps: 41, steps per second: 379, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.123 [-0.711, 0.414], loss: 4.573931, mean_absolute_error: 10.836716, mean_q: 20.865650\n",
      "  5004/50000: episode: 184, duration: 0.099s, episode steps: 39, steps per second: 394, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.139 [-0.870, 0.268], loss: 4.249734, mean_absolute_error: 10.946232, mean_q: 21.116325\n",
      "  5067/50000: episode: 185, duration: 0.162s, episode steps: 63, steps per second: 390, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.061 [-0.810, 0.235], loss: 3.955859, mean_absolute_error: 10.976019, mean_q: 21.293230\n",
      "  5143/50000: episode: 186, duration: 0.194s, episode steps: 76, steps per second: 392, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.987, 0.387], loss: 3.693891, mean_absolute_error: 11.127677, mean_q: 21.612818\n",
      "  5181/50000: episode: 187, duration: 0.098s, episode steps: 38, steps per second: 389, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.121 [-0.800, 0.224], loss: 3.570943, mean_absolute_error: 11.146656, mean_q: 21.613361\n",
      "  5234/50000: episode: 188, duration: 0.136s, episode steps: 53, steps per second: 390, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.095 [-0.682, 0.166], loss: 4.138110, mean_absolute_error: 11.225040, mean_q: 21.696211\n",
      "  5336/50000: episode: 189, duration: 0.254s, episode steps: 102, steps per second: 401, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.029 [-0.739, 0.556], loss: 3.381126, mean_absolute_error: 11.360117, mean_q: 22.051073\n",
      "  5387/50000: episode: 190, duration: 0.130s, episode steps: 51, steps per second: 393, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.098 [-0.745, 0.415], loss: 3.283499, mean_absolute_error: 11.408297, mean_q: 22.154156\n",
      "  5447/50000: episode: 191, duration: 0.151s, episode steps: 60, steps per second: 398, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.054 [-0.924, 0.206], loss: 4.056485, mean_absolute_error: 11.485593, mean_q: 22.246008\n",
      "  5483/50000: episode: 192, duration: 0.093s, episode steps: 36, steps per second: 385, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.106 [-0.675, 0.404], loss: 4.459164, mean_absolute_error: 11.570001, mean_q: 22.351908\n",
      "  5516/50000: episode: 193, duration: 0.085s, episode steps: 33, steps per second: 389, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.129 [-0.654, 0.374], loss: 4.200528, mean_absolute_error: 11.638460, mean_q: 22.493486\n",
      "  5556/50000: episode: 194, duration: 0.101s, episode steps: 40, steps per second: 396, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.119 [-0.665, 0.181], loss: 4.323444, mean_absolute_error: 11.517950, mean_q: 22.293480\n",
      "  5590/50000: episode: 195, duration: 0.085s, episode steps: 34, steps per second: 400, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.111 [-0.811, 0.441], loss: 3.612680, mean_absolute_error: 11.697685, mean_q: 22.761187\n",
      "  5659/50000: episode: 196, duration: 0.174s, episode steps: 69, steps per second: 396, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.124 [-0.525, 0.693], loss: 3.923812, mean_absolute_error: 11.629268, mean_q: 22.566711\n",
      "  5715/50000: episode: 197, duration: 0.139s, episode steps: 56, steps per second: 402, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.110 [-0.764, 0.369], loss: 3.812721, mean_absolute_error: 11.798536, mean_q: 22.957577\n",
      "  5774/50000: episode: 198, duration: 0.150s, episode steps: 59, steps per second: 393, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.086 [-0.869, 0.363], loss: 3.222952, mean_absolute_error: 11.819576, mean_q: 23.115347\n",
      "  5912/50000: episode: 199, duration: 0.351s, episode steps: 138, steps per second: 393, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.141 [-0.452, 0.978], loss: 3.582882, mean_absolute_error: 11.986567, mean_q: 23.397867\n",
      "  5963/50000: episode: 200, duration: 0.131s, episode steps: 51, steps per second: 391, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.134 [-0.754, 0.170], loss: 3.808731, mean_absolute_error: 12.108674, mean_q: 23.641970\n",
      "  6061/50000: episode: 201, duration: 0.244s, episode steps: 98, steps per second: 402, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.041 [-0.876, 0.412], loss: 3.906473, mean_absolute_error: 12.223267, mean_q: 23.875097\n",
      "  6125/50000: episode: 202, duration: 0.161s, episode steps: 64, steps per second: 397, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.118 [-0.774, 0.298], loss: 3.787205, mean_absolute_error: 12.216364, mean_q: 23.890028\n",
      "  6191/50000: episode: 203, duration: 0.166s, episode steps: 66, steps per second: 397, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.117 [-0.911, 0.384], loss: 3.819940, mean_absolute_error: 12.355699, mean_q: 24.196678\n",
      "  6280/50000: episode: 204, duration: 0.227s, episode steps: 89, steps per second: 392, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.087 [-0.860, 0.407], loss: 3.961433, mean_absolute_error: 12.456677, mean_q: 24.427977\n",
      "  6347/50000: episode: 205, duration: 0.171s, episode steps: 67, steps per second: 391, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.119 [-1.117, 0.368], loss: 3.081007, mean_absolute_error: 12.499307, mean_q: 24.632298\n",
      "  6422/50000: episode: 206, duration: 0.196s, episode steps: 75, steps per second: 382, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.121 [-1.044, 0.439], loss: 4.663423, mean_absolute_error: 12.660386, mean_q: 24.769258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6511/50000: episode: 207, duration: 0.230s, episode steps: 89, steps per second: 388, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.142 [-1.293, 0.372], loss: 4.456431, mean_absolute_error: 12.620161, mean_q: 24.670036\n",
      "  6569/50000: episode: 208, duration: 0.143s, episode steps: 58, steps per second: 406, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.165 [-1.095, 0.194], loss: 3.753798, mean_absolute_error: 12.687646, mean_q: 24.958586\n",
      "  6628/50000: episode: 209, duration: 0.155s, episode steps: 59, steps per second: 380, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.153 [-1.231, 0.250], loss: 3.577906, mean_absolute_error: 12.799254, mean_q: 25.198744\n",
      "  6706/50000: episode: 210, duration: 0.196s, episode steps: 78, steps per second: 398, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.125 [-1.134, 0.342], loss: 4.540204, mean_absolute_error: 12.850329, mean_q: 25.162443\n",
      "  6762/50000: episode: 211, duration: 0.154s, episode steps: 56, steps per second: 364, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.167 [-1.322, 0.266], loss: 3.483874, mean_absolute_error: 12.936135, mean_q: 25.470459\n",
      "  6822/50000: episode: 212, duration: 0.151s, episode steps: 60, steps per second: 397, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.167 [-1.126, 0.348], loss: 4.601720, mean_absolute_error: 12.890733, mean_q: 25.176571\n",
      "  6891/50000: episode: 213, duration: 0.166s, episode steps: 69, steps per second: 416, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.204 [-1.610, 0.427], loss: 3.885826, mean_absolute_error: 12.963524, mean_q: 25.420956\n",
      "  6973/50000: episode: 214, duration: 0.209s, episode steps: 82, steps per second: 393, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.165 [-1.446, 0.434], loss: 3.515971, mean_absolute_error: 13.120648, mean_q: 25.795963\n",
      "  7053/50000: episode: 215, duration: 0.197s, episode steps: 80, steps per second: 406, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.244 [-1.797, 0.281], loss: 3.495460, mean_absolute_error: 13.140631, mean_q: 25.832865\n",
      "  7143/50000: episode: 216, duration: 0.234s, episode steps: 90, steps per second: 385, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.225 [-1.822, 0.431], loss: 4.137982, mean_absolute_error: 13.238325, mean_q: 26.009800\n",
      "  7212/50000: episode: 217, duration: 0.171s, episode steps: 69, steps per second: 403, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.167 [-1.305, 0.789], loss: 4.094213, mean_absolute_error: 13.251221, mean_q: 25.962729\n",
      "  7276/50000: episode: 218, duration: 0.164s, episode steps: 64, steps per second: 389, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.219 [-1.509, 0.351], loss: 4.178265, mean_absolute_error: 13.255955, mean_q: 25.996437\n",
      "  7348/50000: episode: 219, duration: 0.176s, episode steps: 72, steps per second: 409, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.301 [-1.995, 0.283], loss: 2.608756, mean_absolute_error: 13.327338, mean_q: 26.276388\n",
      "  7429/50000: episode: 220, duration: 0.206s, episode steps: 81, steps per second: 393, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.248 [-1.640, 0.253], loss: 3.183820, mean_absolute_error: 13.309771, mean_q: 26.237955\n",
      "  7538/50000: episode: 221, duration: 0.274s, episode steps: 109, steps per second: 398, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.360 [-2.410, 0.415], loss: 3.396317, mean_absolute_error: 13.441414, mean_q: 26.494448\n",
      "  7654/50000: episode: 222, duration: 0.284s, episode steps: 116, steps per second: 408, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.356 [-2.205, 0.280], loss: 3.863476, mean_absolute_error: 13.456097, mean_q: 26.535681\n",
      "  7774/50000: episode: 223, duration: 0.298s, episode steps: 120, steps per second: 403, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.311 [-2.143, 0.449], loss: 3.679530, mean_absolute_error: 13.616093, mean_q: 26.769587\n",
      "  7883/50000: episode: 224, duration: 0.284s, episode steps: 109, steps per second: 383, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.325 [-2.023, 0.416], loss: 4.136538, mean_absolute_error: 13.651623, mean_q: 26.748680\n",
      "  7981/50000: episode: 225, duration: 0.248s, episode steps: 98, steps per second: 396, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.395 [-2.136, 0.438], loss: 4.145480, mean_absolute_error: 13.700805, mean_q: 26.904917\n",
      "  8181/50000: episode: 226, duration: 0.502s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.111 [-1.825, 0.451], loss: 3.466426, mean_absolute_error: 13.911831, mean_q: 27.409386\n",
      "  8277/50000: episode: 227, duration: 0.238s, episode steps: 96, steps per second: 403, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.384 [-2.161, 0.417], loss: 4.332785, mean_absolute_error: 13.921292, mean_q: 27.444687\n",
      "  8397/50000: episode: 228, duration: 0.305s, episode steps: 120, steps per second: 394, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.344 [-2.203, 0.380], loss: 3.870708, mean_absolute_error: 14.162508, mean_q: 28.016211\n",
      "  8510/50000: episode: 229, duration: 0.296s, episode steps: 113, steps per second: 382, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.305 [-1.985, 0.536], loss: 3.414931, mean_absolute_error: 14.197914, mean_q: 28.163433\n",
      "  8614/50000: episode: 230, duration: 0.261s, episode steps: 104, steps per second: 398, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.422 [-2.223, 0.338], loss: 3.218629, mean_absolute_error: 14.241347, mean_q: 28.205376\n",
      "  8719/50000: episode: 231, duration: 0.272s, episode steps: 105, steps per second: 386, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.342 [-2.061, 0.299], loss: 3.796560, mean_absolute_error: 14.475691, mean_q: 28.670794\n",
      "  8865/50000: episode: 232, duration: 0.370s, episode steps: 146, steps per second: 394, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.259 [-2.156, 0.419], loss: 3.602050, mean_absolute_error: 14.452486, mean_q: 28.552401\n",
      "  8963/50000: episode: 233, duration: 0.361s, episode steps: 98, steps per second: 271, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.343 [-1.863, 0.382], loss: 3.636212, mean_absolute_error: 14.660221, mean_q: 29.078978\n",
      "  9095/50000: episode: 234, duration: 0.343s, episode steps: 132, steps per second: 385, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.276 [-2.196, 0.474], loss: 3.409919, mean_absolute_error: 14.905037, mean_q: 29.556908\n",
      "  9226/50000: episode: 235, duration: 0.320s, episode steps: 131, steps per second: 410, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.264 [-1.984, 0.506], loss: 3.150220, mean_absolute_error: 15.006598, mean_q: 29.898672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9382/50000: episode: 236, duration: 0.462s, episode steps: 156, steps per second: 337, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.209 [-1.869, 0.613], loss: 4.115394, mean_absolute_error: 15.232283, mean_q: 30.246187\n",
      "  9540/50000: episode: 237, duration: 0.448s, episode steps: 158, steps per second: 353, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.181 [-1.800, 0.478], loss: 4.774404, mean_absolute_error: 15.425368, mean_q: 30.529016\n",
      "  9668/50000: episode: 238, duration: 0.332s, episode steps: 128, steps per second: 386, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.231 [-1.499, 0.477], loss: 3.377115, mean_absolute_error: 15.742575, mean_q: 31.347836\n",
      "  9806/50000: episode: 239, duration: 0.341s, episode steps: 138, steps per second: 405, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.260 [-1.851, 0.440], loss: 4.687844, mean_absolute_error: 15.715024, mean_q: 31.230713\n",
      "  9926/50000: episode: 240, duration: 0.303s, episode steps: 120, steps per second: 396, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.284 [-1.813, 0.491], loss: 4.217641, mean_absolute_error: 16.088928, mean_q: 31.983307\n",
      " 10047/50000: episode: 241, duration: 0.306s, episode steps: 121, steps per second: 395, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.278 [-1.659, 0.455], loss: 4.417213, mean_absolute_error: 16.045315, mean_q: 31.945620\n",
      " 10158/50000: episode: 242, duration: 0.281s, episode steps: 111, steps per second: 395, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.291 [-1.689, 0.612], loss: 4.145967, mean_absolute_error: 16.258892, mean_q: 32.405010\n",
      " 10287/50000: episode: 243, duration: 0.317s, episode steps: 129, steps per second: 407, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.262 [-1.777, 0.561], loss: 4.533121, mean_absolute_error: 16.332607, mean_q: 32.488087\n",
      " 10403/50000: episode: 244, duration: 0.291s, episode steps: 116, steps per second: 398, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.282 [-1.673, 0.439], loss: 3.349316, mean_absolute_error: 16.548588, mean_q: 33.064339\n",
      " 10557/50000: episode: 245, duration: 0.388s, episode steps: 154, steps per second: 397, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.234 [-1.867, 0.466], loss: 3.131382, mean_absolute_error: 16.681017, mean_q: 33.402748\n",
      " 10711/50000: episode: 246, duration: 0.379s, episode steps: 154, steps per second: 406, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.229 [-1.862, 0.466], loss: 3.602774, mean_absolute_error: 16.928209, mean_q: 33.916195\n",
      " 10827/50000: episode: 247, duration: 0.293s, episode steps: 116, steps per second: 396, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.292 [-1.796, 0.517], loss: 4.204024, mean_absolute_error: 17.123726, mean_q: 34.274101\n",
      " 10970/50000: episode: 248, duration: 0.352s, episode steps: 143, steps per second: 406, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.233 [-1.677, 0.426], loss: 3.608308, mean_absolute_error: 17.459015, mean_q: 34.946789\n",
      " 11071/50000: episode: 249, duration: 0.251s, episode steps: 101, steps per second: 402, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.329 [-1.675, 0.604], loss: 4.495466, mean_absolute_error: 17.632914, mean_q: 35.176773\n",
      " 11187/50000: episode: 250, duration: 0.290s, episode steps: 116, steps per second: 400, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.304 [-1.797, 0.509], loss: 4.778337, mean_absolute_error: 17.637733, mean_q: 35.267918\n",
      " 11387/50000: episode: 251, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.102 [-1.294, 0.727], loss: 4.231539, mean_absolute_error: 17.912666, mean_q: 35.883232\n",
      " 11495/50000: episode: 252, duration: 0.274s, episode steps: 108, steps per second: 394, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.304 [-1.826, 0.661], loss: 4.581103, mean_absolute_error: 18.132645, mean_q: 36.300312\n",
      " 11672/50000: episode: 253, duration: 0.440s, episode steps: 177, steps per second: 402, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.222 [-1.975, 0.516], loss: 4.527460, mean_absolute_error: 18.215055, mean_q: 36.513897\n",
      " 11821/50000: episode: 254, duration: 0.375s, episode steps: 149, steps per second: 397, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.203 [-1.646, 0.518], loss: 4.309925, mean_absolute_error: 18.579668, mean_q: 37.244030\n",
      " 11935/50000: episode: 255, duration: 0.292s, episode steps: 114, steps per second: 391, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.315 [-1.787, 0.459], loss: 4.329328, mean_absolute_error: 18.882101, mean_q: 37.850334\n",
      " 12041/50000: episode: 256, duration: 0.262s, episode steps: 106, steps per second: 405, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.312 [-1.787, 0.504], loss: 3.903116, mean_absolute_error: 18.920809, mean_q: 37.986267\n",
      " 12202/50000: episode: 257, duration: 0.399s, episode steps: 161, steps per second: 404, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.218 [-1.777, 0.653], loss: 4.977923, mean_absolute_error: 19.017639, mean_q: 38.173882\n",
      " 12335/50000: episode: 258, duration: 0.330s, episode steps: 133, steps per second: 403, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.286 [-1.798, 0.476], loss: 4.894464, mean_absolute_error: 19.291313, mean_q: 38.750324\n",
      " 12474/50000: episode: 259, duration: 0.395s, episode steps: 139, steps per second: 351, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.271 [-2.013, 0.522], loss: 4.661886, mean_absolute_error: 19.411621, mean_q: 39.091064\n",
      " 12598/50000: episode: 260, duration: 0.306s, episode steps: 124, steps per second: 405, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.312 [-1.838, 0.336], loss: 4.695620, mean_absolute_error: 19.677464, mean_q: 39.694466\n",
      " 12769/50000: episode: 261, duration: 0.421s, episode steps: 171, steps per second: 406, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.240 [-2.018, 0.518], loss: 4.915837, mean_absolute_error: 19.840178, mean_q: 40.001461\n",
      " 12918/50000: episode: 262, duration: 0.427s, episode steps: 149, steps per second: 349, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.284 [-2.031, 0.438], loss: 5.623487, mean_absolute_error: 20.178474, mean_q: 40.622463\n",
      " 13060/50000: episode: 263, duration: 0.351s, episode steps: 142, steps per second: 405, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.264 [-1.833, 0.565], loss: 6.096894, mean_absolute_error: 20.212946, mean_q: 40.751278\n",
      " 13188/50000: episode: 264, duration: 0.318s, episode steps: 128, steps per second: 402, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.307 [-1.841, 0.443], loss: 4.316525, mean_absolute_error: 20.516163, mean_q: 41.441528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13384/50000: episode: 265, duration: 0.588s, episode steps: 196, steps per second: 333, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.197 [-1.869, 0.515], loss: 4.749671, mean_absolute_error: 20.595312, mean_q: 41.641850\n",
      " 13529/50000: episode: 266, duration: 0.361s, episode steps: 145, steps per second: 402, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.283 [-1.995, 0.302], loss: 5.860536, mean_absolute_error: 20.938406, mean_q: 42.226463\n",
      " 13637/50000: episode: 267, duration: 0.268s, episode steps: 108, steps per second: 403, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.328 [-1.838, 0.334], loss: 4.798449, mean_absolute_error: 21.049585, mean_q: 42.603424\n",
      " 13777/50000: episode: 268, duration: 0.354s, episode steps: 140, steps per second: 395, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.280 [-1.830, 0.577], loss: 5.234366, mean_absolute_error: 21.494263, mean_q: 43.462032\n",
      " 13905/50000: episode: 269, duration: 0.320s, episode steps: 128, steps per second: 401, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.316 [-1.844, 0.331], loss: 4.778477, mean_absolute_error: 21.502972, mean_q: 43.656151\n",
      " 14060/50000: episode: 270, duration: 0.384s, episode steps: 155, steps per second: 404, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.219 [-1.633, 0.389], loss: 5.309314, mean_absolute_error: 21.600355, mean_q: 43.711254\n",
      " 14189/50000: episode: 271, duration: 0.321s, episode steps: 129, steps per second: 402, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.313 [-2.005, 0.592], loss: 5.534680, mean_absolute_error: 21.950270, mean_q: 44.353012\n",
      " 14329/50000: episode: 272, duration: 0.344s, episode steps: 140, steps per second: 407, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.268 [-1.804, 0.459], loss: 5.843677, mean_absolute_error: 22.069355, mean_q: 44.661549\n",
      " 14490/50000: episode: 273, duration: 0.406s, episode steps: 161, steps per second: 397, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.261 [-2.038, 0.580], loss: 5.432200, mean_absolute_error: 22.248426, mean_q: 45.041908\n",
      " 14623/50000: episode: 274, duration: 0.350s, episode steps: 133, steps per second: 380, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.315 [-2.030, 0.520], loss: 4.372658, mean_absolute_error: 22.366354, mean_q: 45.472012\n",
      " 14818/50000: episode: 275, duration: 0.600s, episode steps: 195, steps per second: 325, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.221 [-1.983, 0.611], loss: 5.853483, mean_absolute_error: 22.794060, mean_q: 46.252594\n",
      " 14974/50000: episode: 276, duration: 0.429s, episode steps: 156, steps per second: 364, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.268 [-1.853, 0.356], loss: 5.761936, mean_absolute_error: 22.914034, mean_q: 46.483006\n",
      " 15174/50000: episode: 277, duration: 0.587s, episode steps: 200, steps per second: 341, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.183 [-1.676, 0.556], loss: 6.106296, mean_absolute_error: 23.312134, mean_q: 47.321457\n",
      " 15299/50000: episode: 278, duration: 0.317s, episode steps: 125, steps per second: 394, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.230 [-1.238, 0.500], loss: 6.240304, mean_absolute_error: 23.598568, mean_q: 47.924709\n",
      " 15499/50000: episode: 279, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.188 [-1.682, 0.473], loss: 6.395537, mean_absolute_error: 23.629028, mean_q: 47.932205\n",
      " 15683/50000: episode: 280, duration: 0.458s, episode steps: 184, steps per second: 402, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.250 [-2.182, 0.581], loss: 5.841518, mean_absolute_error: 23.986282, mean_q: 48.775784\n",
      " 15808/50000: episode: 281, duration: 0.382s, episode steps: 125, steps per second: 327, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.355 [-2.145, 0.358], loss: 7.208924, mean_absolute_error: 24.146502, mean_q: 48.974609\n",
      " 15975/50000: episode: 282, duration: 0.430s, episode steps: 167, steps per second: 389, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.292 [-2.226, 0.585], loss: 6.677974, mean_absolute_error: 24.173855, mean_q: 49.110779\n",
      " 16170/50000: episode: 283, duration: 0.485s, episode steps: 195, steps per second: 402, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.239 [-2.048, 0.579], loss: 5.710810, mean_absolute_error: 24.680862, mean_q: 50.260025\n",
      " 16339/50000: episode: 284, duration: 0.427s, episode steps: 169, steps per second: 396, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.274 [-2.017, 0.480], loss: 4.780931, mean_absolute_error: 24.893082, mean_q: 50.798748\n",
      " 16495/50000: episode: 285, duration: 0.394s, episode steps: 156, steps per second: 396, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.275 [-1.799, 0.479], loss: 6.589808, mean_absolute_error: 25.221445, mean_q: 51.335625\n",
      " 16690/50000: episode: 286, duration: 0.590s, episode steps: 195, steps per second: 330, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.222 [-1.856, 0.578], loss: 6.493180, mean_absolute_error: 25.607182, mean_q: 52.113220\n",
      " 16846/50000: episode: 287, duration: 0.395s, episode steps: 156, steps per second: 395, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.265 [-1.860, 0.508], loss: 7.254305, mean_absolute_error: 25.759672, mean_q: 52.359699\n",
      " 17002/50000: episode: 288, duration: 0.399s, episode steps: 156, steps per second: 391, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.284 [-1.824, 0.402], loss: 4.773503, mean_absolute_error: 25.974415, mean_q: 53.011745\n",
      " 17168/50000: episode: 289, duration: 0.513s, episode steps: 166, steps per second: 324, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.207 [-1.615, 0.546], loss: 7.435603, mean_absolute_error: 26.019419, mean_q: 52.949829\n",
      " 17334/50000: episode: 290, duration: 0.490s, episode steps: 166, steps per second: 339, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.257 [-1.835, 0.411], loss: 6.548914, mean_absolute_error: 26.411968, mean_q: 53.673309\n",
      " 17518/50000: episode: 291, duration: 0.514s, episode steps: 184, steps per second: 358, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.245 [-1.960, 0.433], loss: 7.378237, mean_absolute_error: 26.569458, mean_q: 53.991768\n",
      " 17647/50000: episode: 292, duration: 0.442s, episode steps: 129, steps per second: 292, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.350 [-2.025, 0.351], loss: 7.030832, mean_absolute_error: 26.773870, mean_q: 54.500065\n",
      " 17829/50000: episode: 293, duration: 0.642s, episode steps: 182, steps per second: 283, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.254 [-1.972, 0.478], loss: 5.950655, mean_absolute_error: 26.885527, mean_q: 54.759537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17987/50000: episode: 294, duration: 0.659s, episode steps: 158, steps per second: 240, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.264 [-1.841, 0.441], loss: 6.520350, mean_absolute_error: 27.130466, mean_q: 55.135216\n",
      " 18116/50000: episode: 295, duration: 0.323s, episode steps: 129, steps per second: 400, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.366 [-2.247, 0.370], loss: 6.284785, mean_absolute_error: 27.494701, mean_q: 55.851261\n",
      " 18265/50000: episode: 296, duration: 0.372s, episode steps: 149, steps per second: 401, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.317 [-2.210, 0.393], loss: 5.907386, mean_absolute_error: 27.524906, mean_q: 55.976719\n",
      " 18442/50000: episode: 297, duration: 0.433s, episode steps: 177, steps per second: 409, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.258 [-2.010, 0.515], loss: 6.492405, mean_absolute_error: 27.486588, mean_q: 55.948448\n",
      " 18622/50000: episode: 298, duration: 0.526s, episode steps: 180, steps per second: 342, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.249 [-2.140, 0.705], loss: 7.086814, mean_absolute_error: 27.788004, mean_q: 56.492405\n",
      " 18778/50000: episode: 299, duration: 0.396s, episode steps: 156, steps per second: 394, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.307 [-2.152, 0.480], loss: 5.845797, mean_absolute_error: 27.855967, mean_q: 56.635841\n",
      " 18931/50000: episode: 300, duration: 0.395s, episode steps: 153, steps per second: 387, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.284 [-1.965, 0.430], loss: 6.124319, mean_absolute_error: 28.170172, mean_q: 57.297550\n",
      " 19085/50000: episode: 301, duration: 0.532s, episode steps: 154, steps per second: 290, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.306 [-2.140, 0.438], loss: 8.193567, mean_absolute_error: 28.280571, mean_q: 57.306297\n",
      " 19230/50000: episode: 302, duration: 0.461s, episode steps: 145, steps per second: 314, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.319 [-2.018, 0.486], loss: 10.521758, mean_absolute_error: 28.110456, mean_q: 56.958950\n",
      " 19356/50000: episode: 303, duration: 0.337s, episode steps: 126, steps per second: 374, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.332 [-1.864, 0.332], loss: 6.443627, mean_absolute_error: 28.390951, mean_q: 57.635303\n",
      " 19498/50000: episode: 304, duration: 0.356s, episode steps: 142, steps per second: 398, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.286 [-1.783, 0.763], loss: 6.304310, mean_absolute_error: 28.420446, mean_q: 57.667198\n",
      " 19662/50000: episode: 305, duration: 0.477s, episode steps: 164, steps per second: 344, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.241 [-1.846, 0.499], loss: 5.528867, mean_absolute_error: 28.489126, mean_q: 57.831150\n",
      " 19844/50000: episode: 306, duration: 0.475s, episode steps: 182, steps per second: 383, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.197 [-1.786, 0.753], loss: 6.802784, mean_absolute_error: 28.935324, mean_q: 58.604679\n",
      " 19996/50000: episode: 307, duration: 0.375s, episode steps: 152, steps per second: 405, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.303 [-2.140, 0.633], loss: 7.073322, mean_absolute_error: 28.560293, mean_q: 57.858040\n",
      " 20131/50000: episode: 308, duration: 0.333s, episode steps: 135, steps per second: 405, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.302 [-2.021, 0.417], loss: 9.458963, mean_absolute_error: 28.883116, mean_q: 58.447235\n",
      " 20319/50000: episode: 309, duration: 0.482s, episode steps: 188, steps per second: 390, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.249 [-2.153, 0.420], loss: 6.510068, mean_absolute_error: 29.089766, mean_q: 58.986618\n",
      " 20441/50000: episode: 310, duration: 0.328s, episode steps: 122, steps per second: 372, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.343 [-2.020, 0.466], loss: 8.489375, mean_absolute_error: 28.936968, mean_q: 58.635487\n",
      " 20590/50000: episode: 311, duration: 0.393s, episode steps: 149, steps per second: 379, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.296 [-2.002, 0.415], loss: 7.494503, mean_absolute_error: 29.363091, mean_q: 59.582615\n",
      " 20719/50000: episode: 312, duration: 0.340s, episode steps: 129, steps per second: 379, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.314 [-1.961, 0.632], loss: 6.854445, mean_absolute_error: 28.980350, mean_q: 58.754036\n",
      " 20877/50000: episode: 313, duration: 0.445s, episode steps: 158, steps per second: 355, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.299 [-2.186, 0.402], loss: 8.511442, mean_absolute_error: 29.332523, mean_q: 59.372322\n",
      " 21027/50000: episode: 314, duration: 0.412s, episode steps: 150, steps per second: 365, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.264 [-1.781, 0.450], loss: 6.428729, mean_absolute_error: 29.272444, mean_q: 59.442669\n",
      " 21227/50000: episode: 315, duration: 0.525s, episode steps: 200, steps per second: 381, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.213 [-1.974, 0.449], loss: 8.402901, mean_absolute_error: 29.573437, mean_q: 59.972725\n",
      " 21415/50000: episode: 316, duration: 0.477s, episode steps: 188, steps per second: 394, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.203 [-1.774, 0.431], loss: 7.117273, mean_absolute_error: 29.747402, mean_q: 60.297440\n",
      " 21539/50000: episode: 317, duration: 0.304s, episode steps: 124, steps per second: 408, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.306 [-1.854, 0.546], loss: 6.812386, mean_absolute_error: 30.037851, mean_q: 60.884411\n",
      " 21739/50000: episode: 318, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.150 [-1.278, 0.439], loss: 7.895155, mean_absolute_error: 29.950571, mean_q: 60.717724\n",
      " 21920/50000: episode: 319, duration: 0.535s, episode steps: 181, steps per second: 338, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.235 [-1.958, 0.514], loss: 7.087111, mean_absolute_error: 30.493790, mean_q: 61.856182\n",
      " 22045/50000: episode: 320, duration: 0.324s, episode steps: 125, steps per second: 386, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.294 [-1.626, 0.693], loss: 7.228932, mean_absolute_error: 30.558098, mean_q: 61.988426\n",
      " 22245/50000: episode: 321, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.087 [-1.065, 0.492], loss: 8.403062, mean_absolute_error: 30.682686, mean_q: 62.078091\n",
      " 22411/50000: episode: 322, duration: 0.419s, episode steps: 166, steps per second: 396, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.250 [-1.782, 0.571], loss: 7.897515, mean_absolute_error: 31.026779, mean_q: 62.747589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22589/50000: episode: 323, duration: 0.448s, episode steps: 178, steps per second: 398, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.231 [-2.010, 0.450], loss: 7.331310, mean_absolute_error: 30.680487, mean_q: 62.017239\n",
      " 22750/50000: episode: 324, duration: 0.420s, episode steps: 161, steps per second: 383, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.281 [-2.034, 0.641], loss: 9.077161, mean_absolute_error: 30.757631, mean_q: 62.071060\n",
      " 22950/50000: episode: 325, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.020 [-0.592, 0.500], loss: 8.728942, mean_absolute_error: 31.147163, mean_q: 62.927231\n",
      " 23086/50000: episode: 326, duration: 0.335s, episode steps: 136, steps per second: 406, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.283 [-1.775, 0.442], loss: 8.301200, mean_absolute_error: 31.050558, mean_q: 62.604321\n",
      " 23286/50000: episode: 327, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.142 [-1.433, 0.456], loss: 6.890945, mean_absolute_error: 31.192045, mean_q: 62.937450\n",
      " 23433/50000: episode: 328, duration: 0.431s, episode steps: 147, steps per second: 341, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.297 [-1.821, 0.428], loss: 5.918030, mean_absolute_error: 31.193520, mean_q: 63.107128\n",
      " 23605/50000: episode: 329, duration: 0.447s, episode steps: 172, steps per second: 385, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.245 [-1.811, 0.623], loss: 7.935046, mean_absolute_error: 31.276310, mean_q: 63.247364\n",
      " 23774/50000: episode: 330, duration: 0.514s, episode steps: 169, steps per second: 329, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.237 [-1.682, 0.355], loss: 8.337228, mean_absolute_error: 31.553682, mean_q: 63.897774\n",
      " 23974/50000: episode: 331, duration: 0.815s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.133 [-1.324, 0.626], loss: 7.598248, mean_absolute_error: 31.430748, mean_q: 63.663746\n",
      " 24149/50000: episode: 332, duration: 0.545s, episode steps: 175, steps per second: 321, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.242 [-1.667, 0.491], loss: 5.650426, mean_absolute_error: 31.808002, mean_q: 64.277252\n",
      " 24307/50000: episode: 333, duration: 0.457s, episode steps: 158, steps per second: 346, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.268 [-1.825, 0.416], loss: 8.896913, mean_absolute_error: 32.088509, mean_q: 64.765305\n",
      " 24464/50000: episode: 334, duration: 0.391s, episode steps: 157, steps per second: 401, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.292 [-2.045, 0.478], loss: 9.566899, mean_absolute_error: 31.998905, mean_q: 64.494316\n",
      " 24616/50000: episode: 335, duration: 0.472s, episode steps: 152, steps per second: 322, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.280 [-1.813, 0.501], loss: 7.891195, mean_absolute_error: 32.252106, mean_q: 65.146469\n",
      " 24816/50000: episode: 336, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.126 [-1.153, 0.545], loss: 7.623516, mean_absolute_error: 31.848583, mean_q: 64.275818\n",
      " 25007/50000: episode: 337, duration: 0.523s, episode steps: 191, steps per second: 365, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.213 [-1.984, 0.635], loss: 8.567537, mean_absolute_error: 32.073029, mean_q: 64.513809\n",
      " 25207/50000: episode: 338, duration: 0.547s, episode steps: 200, steps per second: 366, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-0.662, 0.513], loss: 7.316931, mean_absolute_error: 32.342884, mean_q: 65.218735\n",
      " 25398/50000: episode: 339, duration: 0.478s, episode steps: 191, steps per second: 400, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.227 [-2.001, 0.522], loss: 8.415936, mean_absolute_error: 32.534061, mean_q: 65.541306\n",
      " 25598/50000: episode: 340, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.575, 0.627], loss: 9.875666, mean_absolute_error: 32.571415, mean_q: 65.575272\n",
      " 25798/50000: episode: 341, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.086 [-0.907, 0.529], loss: 9.499112, mean_absolute_error: 32.710289, mean_q: 65.785355\n",
      " 25957/50000: episode: 342, duration: 1.296s, episode steps: 159, steps per second: 123, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.271 [-2.014, 0.700], loss: 8.688916, mean_absolute_error: 32.874565, mean_q: 65.949272\n",
      " 26121/50000: episode: 343, duration: 1.356s, episode steps: 164, steps per second: 121, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.237 [-1.849, 0.454], loss: 8.412227, mean_absolute_error: 32.816593, mean_q: 65.844994\n",
      " 26278/50000: episode: 344, duration: 1.266s, episode steps: 157, steps per second: 124, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.277 [-1.893, 0.832], loss: 7.252664, mean_absolute_error: 32.867393, mean_q: 66.107040\n",
      " 26450/50000: episode: 345, duration: 1.369s, episode steps: 172, steps per second: 126, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.255 [-1.806, 0.562], loss: 7.490862, mean_absolute_error: 33.273243, mean_q: 67.131882\n",
      " 26613/50000: episode: 346, duration: 1.360s, episode steps: 163, steps per second: 120, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.269 [-1.957, 0.619], loss: 8.177511, mean_absolute_error: 33.259418, mean_q: 66.985222\n",
      " 26813/50000: episode: 347, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.009 [-0.870, 0.829], loss: 8.525266, mean_absolute_error: 33.190010, mean_q: 66.790672\n",
      " 26978/50000: episode: 348, duration: 1.028s, episode steps: 165, steps per second: 160, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.265 [-1.967, 0.453], loss: 8.131250, mean_absolute_error: 33.550674, mean_q: 67.592613\n",
      " 27169/50000: episode: 349, duration: 1.172s, episode steps: 191, steps per second: 163, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.220 [-1.619, 0.391], loss: 10.660281, mean_absolute_error: 33.464840, mean_q: 67.208717\n",
      " 27314/50000: episode: 350, duration: 0.919s, episode steps: 145, steps per second: 158, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.301 [-1.675, 0.388], loss: 10.538693, mean_absolute_error: 33.670300, mean_q: 67.717117\n",
      " 27502/50000: episode: 351, duration: 1.184s, episode steps: 188, steps per second: 159, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.227 [-1.828, 0.452], loss: 10.299232, mean_absolute_error: 33.569904, mean_q: 67.352417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27657/50000: episode: 352, duration: 0.987s, episode steps: 155, steps per second: 157, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.267 [-1.831, 0.516], loss: 5.874833, mean_absolute_error: 33.544415, mean_q: 67.470963\n",
      " 27857/50000: episode: 353, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.093 [-0.939, 0.505], loss: 9.220509, mean_absolute_error: 33.834423, mean_q: 68.120956\n",
      " 28038/50000: episode: 354, duration: 1.147s, episode steps: 181, steps per second: 158, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.246 [-1.966, 0.453], loss: 6.445538, mean_absolute_error: 33.938950, mean_q: 68.347557\n",
      " 28198/50000: episode: 355, duration: 1.015s, episode steps: 160, steps per second: 158, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.270 [-1.843, 0.572], loss: 7.954270, mean_absolute_error: 34.015911, mean_q: 68.485703\n",
      " 28333/50000: episode: 356, duration: 0.895s, episode steps: 135, steps per second: 151, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.292 [-1.619, 0.380], loss: 8.045383, mean_absolute_error: 33.862724, mean_q: 68.109962\n",
      " 28506/50000: episode: 357, duration: 1.116s, episode steps: 173, steps per second: 155, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.263 [-1.955, 0.570], loss: 8.216746, mean_absolute_error: 34.423367, mean_q: 69.291374\n",
      " 28651/50000: episode: 358, duration: 0.891s, episode steps: 145, steps per second: 163, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.273 [-1.674, 0.554], loss: 7.235857, mean_absolute_error: 34.159401, mean_q: 68.832191\n",
      " 28831/50000: episode: 359, duration: 1.206s, episode steps: 180, steps per second: 149, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.271 [-2.167, 0.489], loss: 8.733158, mean_absolute_error: 34.259853, mean_q: 68.939819\n",
      " 28978/50000: episode: 360, duration: 0.927s, episode steps: 147, steps per second: 159, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.303 [-1.960, 0.562], loss: 8.138275, mean_absolute_error: 34.168388, mean_q: 68.714317\n",
      " 29136/50000: episode: 361, duration: 1.063s, episode steps: 158, steps per second: 149, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.278 [-1.847, 0.524], loss: 6.576028, mean_absolute_error: 34.338047, mean_q: 69.170105\n",
      " 29282/50000: episode: 362, duration: 0.873s, episode steps: 146, steps per second: 167, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.292 [-1.860, 0.356], loss: 7.842860, mean_absolute_error: 34.395962, mean_q: 69.290894\n",
      " 29482/50000: episode: 363, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.064 [-0.765, 0.556], loss: 7.787100, mean_absolute_error: 34.365585, mean_q: 69.113014\n",
      " 29642/50000: episode: 364, duration: 1.001s, episode steps: 160, steps per second: 160, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.267 [-1.779, 0.429], loss: 11.349649, mean_absolute_error: 34.594612, mean_q: 69.369339\n",
      " 29815/50000: episode: 365, duration: 1.080s, episode steps: 173, steps per second: 160, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.262 [-1.960, 0.536], loss: 9.651479, mean_absolute_error: 34.347893, mean_q: 68.897644\n",
      " 29968/50000: episode: 366, duration: 1.014s, episode steps: 153, steps per second: 151, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.271 [-1.992, 0.524], loss: 8.436893, mean_absolute_error: 34.163742, mean_q: 68.581657\n",
      " 30162/50000: episode: 367, duration: 1.339s, episode steps: 194, steps per second: 145, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.243 [-2.141, 0.812], loss: 7.892101, mean_absolute_error: 34.557022, mean_q: 69.423042\n",
      " 30322/50000: episode: 368, duration: 0.901s, episode steps: 160, steps per second: 178, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.279 [-1.823, 0.395], loss: 6.443991, mean_absolute_error: 34.162544, mean_q: 68.831589\n",
      " 30522/50000: episode: 369, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.207 [-1.783, 0.521], loss: 7.786343, mean_absolute_error: 34.254086, mean_q: 68.722374\n",
      " 30699/50000: episode: 370, duration: 0.840s, episode steps: 177, steps per second: 211, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.246 [-1.857, 0.481], loss: 6.821455, mean_absolute_error: 34.435822, mean_q: 69.024483\n",
      " 30843/50000: episode: 371, duration: 0.673s, episode steps: 144, steps per second: 214, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.281 [-1.806, 0.520], loss: 9.698180, mean_absolute_error: 34.325089, mean_q: 68.641457\n",
      " 30987/50000: episode: 372, duration: 0.673s, episode steps: 144, steps per second: 214, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.293 [-1.659, 0.461], loss: 7.744764, mean_absolute_error: 34.394958, mean_q: 69.001060\n",
      " 31149/50000: episode: 373, duration: 0.763s, episode steps: 162, steps per second: 212, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.262 [-1.798, 0.622], loss: 6.501018, mean_absolute_error: 34.363853, mean_q: 69.027275\n",
      " 31320/50000: episode: 374, duration: 0.810s, episode steps: 171, steps per second: 211, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.266 [-1.994, 0.545], loss: 6.737082, mean_absolute_error: 34.341690, mean_q: 68.814354\n",
      " 31519/50000: episode: 375, duration: 0.941s, episode steps: 199, steps per second: 211, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.212 [-1.787, 0.616], loss: 7.010911, mean_absolute_error: 34.625172, mean_q: 69.500626\n",
      " 31719/50000: episode: 376, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.197 [-1.869, 0.646], loss: 8.603863, mean_absolute_error: 34.394859, mean_q: 68.978889\n",
      " 31919/50000: episode: 377, duration: 1.016s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.162 [-1.507, 0.733], loss: 7.484091, mean_absolute_error: 34.504436, mean_q: 69.065750\n",
      " 32119/50000: episode: 378, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.018 [-0.580, 0.570], loss: 9.227868, mean_absolute_error: 34.398926, mean_q: 68.731125\n",
      " 32271/50000: episode: 379, duration: 0.732s, episode steps: 152, steps per second: 208, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.291 [-1.817, 0.568], loss: 10.319799, mean_absolute_error: 35.073936, mean_q: 70.341492\n",
      " 32471/50000: episode: 380, duration: 0.962s, episode steps: 200, steps per second: 208, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-0.742, 0.582], loss: 7.598940, mean_absolute_error: 34.244289, mean_q: 68.608810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32633/50000: episode: 381, duration: 0.774s, episode steps: 162, steps per second: 209, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.275 [-1.868, 0.537], loss: 7.188449, mean_absolute_error: 34.727104, mean_q: 69.449280\n",
      " 32780/50000: episode: 382, duration: 0.713s, episode steps: 147, steps per second: 206, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.299 [-2.020, 0.353], loss: 6.973725, mean_absolute_error: 34.670589, mean_q: 69.621452\n",
      " 32910/50000: episode: 383, duration: 0.614s, episode steps: 130, steps per second: 212, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.341 [-1.828, 0.466], loss: 6.140037, mean_absolute_error: 34.654854, mean_q: 69.308296\n",
      " 33108/50000: episode: 384, duration: 0.946s, episode steps: 198, steps per second: 209, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.211 [-1.869, 0.636], loss: 11.097485, mean_absolute_error: 34.570683, mean_q: 69.138748\n",
      " 33231/50000: episode: 385, duration: 0.573s, episode steps: 123, steps per second: 215, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.349 [-2.004, 0.763], loss: 10.109467, mean_absolute_error: 34.742722, mean_q: 69.642914\n",
      " 33396/50000: episode: 386, duration: 1.030s, episode steps: 165, steps per second: 160, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.276 [-1.960, 0.525], loss: 6.476508, mean_absolute_error: 34.577827, mean_q: 69.173912\n",
      " 33570/50000: episode: 387, duration: 0.967s, episode steps: 174, steps per second: 180, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.243 [-1.642, 0.549], loss: 7.129835, mean_absolute_error: 34.619461, mean_q: 69.267700\n",
      " 33770/50000: episode: 388, duration: 0.771s, episode steps: 200, steps per second: 259, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.080 [-0.786, 0.572], loss: 8.384085, mean_absolute_error: 34.763504, mean_q: 69.414001\n",
      " 33917/50000: episode: 389, duration: 0.476s, episode steps: 147, steps per second: 309, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.324 [-2.039, 0.484], loss: 9.357400, mean_absolute_error: 34.637203, mean_q: 69.380028\n",
      " 34048/50000: episode: 390, duration: 0.429s, episode steps: 131, steps per second: 305, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.337 [-1.855, 0.500], loss: 7.093446, mean_absolute_error: 34.779194, mean_q: 69.716866\n",
      " 34220/50000: episode: 391, duration: 0.941s, episode steps: 172, steps per second: 183, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.254 [-1.793, 0.410], loss: 6.909661, mean_absolute_error: 35.004147, mean_q: 70.224365\n",
      " 34392/50000: episode: 392, duration: 0.539s, episode steps: 172, steps per second: 319, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.256 [-1.790, 0.423], loss: 8.678338, mean_absolute_error: 34.651508, mean_q: 69.314247\n",
      " 34592/50000: episode: 393, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.110 [-1.082, 0.661], loss: 7.919844, mean_absolute_error: 34.959583, mean_q: 69.866821\n",
      " 34792/50000: episode: 394, duration: 0.596s, episode steps: 200, steps per second: 336, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.199 [-1.677, 0.532], loss: 8.556718, mean_absolute_error: 35.070457, mean_q: 70.356308\n",
      " 34946/50000: episode: 395, duration: 0.443s, episode steps: 154, steps per second: 348, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.279 [-1.832, 0.371], loss: 7.311885, mean_absolute_error: 35.129414, mean_q: 70.310905\n",
      " 35146/50000: episode: 396, duration: 0.397s, episode steps: 200, steps per second: 504, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.166 [-1.475, 0.534], loss: 6.558898, mean_absolute_error: 34.904755, mean_q: 69.938690\n",
      " 35315/50000: episode: 397, duration: 0.424s, episode steps: 169, steps per second: 399, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.281 [-1.966, 0.536], loss: 6.619438, mean_absolute_error: 35.064014, mean_q: 70.202850\n",
      " 35515/50000: episode: 398, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.186 [-1.627, 0.463], loss: 9.134474, mean_absolute_error: 34.872528, mean_q: 69.711472\n",
      " 35715/50000: episode: 399, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.117 [-1.133, 0.424], loss: 8.967294, mean_absolute_error: 35.083973, mean_q: 70.260300\n",
      " 35873/50000: episode: 400, duration: 0.308s, episode steps: 158, steps per second: 513, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.283 [-1.800, 0.547], loss: 3.665321, mean_absolute_error: 35.106647, mean_q: 70.337151\n",
      " 36045/50000: episode: 401, duration: 0.422s, episode steps: 172, steps per second: 407, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.267 [-1.865, 0.504], loss: 5.951300, mean_absolute_error: 34.678383, mean_q: 69.156616\n",
      " 36202/50000: episode: 402, duration: 0.407s, episode steps: 157, steps per second: 386, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.268 [-1.855, 0.375], loss: 7.190018, mean_absolute_error: 34.900364, mean_q: 69.724808\n",
      " 36360/50000: episode: 403, duration: 0.328s, episode steps: 158, steps per second: 482, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.280 [-1.808, 0.793], loss: 8.506731, mean_absolute_error: 34.556938, mean_q: 68.905457\n",
      " 36519/50000: episode: 404, duration: 0.401s, episode steps: 159, steps per second: 396, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.265 [-2.031, 0.476], loss: 7.059902, mean_absolute_error: 34.784904, mean_q: 69.576820\n",
      " 36706/50000: episode: 405, duration: 0.521s, episode steps: 187, steps per second: 359, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.255 [-1.989, 0.414], loss: 6.294161, mean_absolute_error: 35.132034, mean_q: 70.436157\n",
      " 36867/50000: episode: 406, duration: 0.434s, episode steps: 161, steps per second: 371, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.283 [-1.999, 0.400], loss: 7.900863, mean_absolute_error: 34.684921, mean_q: 69.319122\n",
      " 37003/50000: episode: 407, duration: 0.313s, episode steps: 136, steps per second: 435, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.310 [-1.853, 0.389], loss: 8.675177, mean_absolute_error: 35.188042, mean_q: 70.536491\n",
      " 37183/50000: episode: 408, duration: 0.500s, episode steps: 180, steps per second: 360, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.246 [-1.834, 0.544], loss: 5.490592, mean_absolute_error: 35.067909, mean_q: 70.269043\n",
      " 37353/50000: episode: 409, duration: 0.330s, episode steps: 170, steps per second: 515, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.279 [-1.848, 0.535], loss: 9.532484, mean_absolute_error: 34.886608, mean_q: 69.651703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37507/50000: episode: 410, duration: 0.352s, episode steps: 154, steps per second: 437, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.291 [-1.854, 0.490], loss: 6.415348, mean_absolute_error: 35.075874, mean_q: 70.157570\n",
      " 37662/50000: episode: 411, duration: 0.420s, episode steps: 155, steps per second: 369, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.302 [-1.987, 0.651], loss: 5.531255, mean_absolute_error: 35.294353, mean_q: 70.656525\n",
      " 37840/50000: episode: 412, duration: 0.458s, episode steps: 178, steps per second: 389, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.212 [-1.458, 0.672], loss: 8.753805, mean_absolute_error: 34.940079, mean_q: 69.804100\n",
      " 38040/50000: episode: 413, duration: 0.382s, episode steps: 200, steps per second: 523, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-0.631, 0.581], loss: 7.776464, mean_absolute_error: 34.893185, mean_q: 69.708801\n",
      " 38240/50000: episode: 414, duration: 0.393s, episode steps: 200, steps per second: 508, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.103 [-0.633, 0.618], loss: 7.842299, mean_absolute_error: 35.125542, mean_q: 70.173050\n",
      " 38434/50000: episode: 415, duration: 0.460s, episode steps: 194, steps per second: 422, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.233 [-1.831, 0.552], loss: 6.319979, mean_absolute_error: 35.327431, mean_q: 70.584938\n",
      " 38601/50000: episode: 416, duration: 0.410s, episode steps: 167, steps per second: 407, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.271 [-1.982, 0.751], loss: 6.450008, mean_absolute_error: 35.381943, mean_q: 70.561287\n",
      " 38801/50000: episode: 417, duration: 0.392s, episode steps: 200, steps per second: 510, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.060 [-0.726, 0.455], loss: 7.249474, mean_absolute_error: 35.171066, mean_q: 70.138313\n",
      " 39001/50000: episode: 418, duration: 0.394s, episode steps: 200, steps per second: 508, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.614, 0.657], loss: 9.288266, mean_absolute_error: 35.003735, mean_q: 69.738556\n",
      " 39201/50000: episode: 419, duration: 0.394s, episode steps: 200, steps per second: 507, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.566, 0.848], loss: 6.152383, mean_absolute_error: 35.154572, mean_q: 70.317619\n",
      " 39370/50000: episode: 420, duration: 0.343s, episode steps: 169, steps per second: 493, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.277 [-2.034, 0.472], loss: 6.548249, mean_absolute_error: 35.268681, mean_q: 70.540405\n",
      " 39570/50000: episode: 421, duration: 0.399s, episode steps: 200, steps per second: 501, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.561, 0.519], loss: 7.300763, mean_absolute_error: 35.283531, mean_q: 70.586235\n",
      " 39770/50000: episode: 422, duration: 0.390s, episode steps: 200, steps per second: 513, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.792, 0.625], loss: 7.511952, mean_absolute_error: 35.385094, mean_q: 70.641998\n",
      " 39938/50000: episode: 423, duration: 0.325s, episode steps: 168, steps per second: 516, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.289 [-2.007, 0.501], loss: 9.480162, mean_absolute_error: 35.569187, mean_q: 71.122536\n",
      " 40138/50000: episode: 424, duration: 0.382s, episode steps: 200, steps per second: 524, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.119 [-1.296, 0.484], loss: 6.439455, mean_absolute_error: 35.764114, mean_q: 71.616165\n",
      " 40276/50000: episode: 425, duration: 0.277s, episode steps: 138, steps per second: 499, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.298 [-1.822, 0.536], loss: 11.665007, mean_absolute_error: 35.481438, mean_q: 70.661179\n",
      " 40429/50000: episode: 426, duration: 0.310s, episode steps: 153, steps per second: 494, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.308 [-2.016, 0.473], loss: 6.279960, mean_absolute_error: 35.276398, mean_q: 70.405457\n",
      " 40615/50000: episode: 427, duration: 0.375s, episode steps: 186, steps per second: 496, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.236 [-1.794, 0.547], loss: 6.948334, mean_absolute_error: 35.390362, mean_q: 70.545334\n",
      " 40815/50000: episode: 428, duration: 0.385s, episode steps: 200, steps per second: 519, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.026 [-0.748, 0.581], loss: 6.737618, mean_absolute_error: 35.640465, mean_q: 71.275421\n",
      " 41015/50000: episode: 429, duration: 0.384s, episode steps: 200, steps per second: 521, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.589, 0.584], loss: 6.850496, mean_absolute_error: 35.668327, mean_q: 71.264626\n",
      " 41160/50000: episode: 430, duration: 0.284s, episode steps: 145, steps per second: 511, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.342 [-2.009, 0.615], loss: 9.662909, mean_absolute_error: 35.510067, mean_q: 70.964058\n",
      " 41347/50000: episode: 431, duration: 0.363s, episode steps: 187, steps per second: 515, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.268 [-2.020, 0.578], loss: 5.846164, mean_absolute_error: 35.613499, mean_q: 71.149132\n",
      " 41547/50000: episode: 432, duration: 0.388s, episode steps: 200, steps per second: 515, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.177 [-1.601, 0.552], loss: 7.852584, mean_absolute_error: 35.785553, mean_q: 71.255013\n",
      " 41747/50000: episode: 433, duration: 0.387s, episode steps: 200, steps per second: 517, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.144 [-1.313, 0.691], loss: 7.311081, mean_absolute_error: 35.700165, mean_q: 71.232422\n",
      " 41917/50000: episode: 434, duration: 0.333s, episode steps: 170, steps per second: 511, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.272 [-1.853, 0.555], loss: 8.967502, mean_absolute_error: 35.795841, mean_q: 71.419945\n",
      " 42117/50000: episode: 435, duration: 0.395s, episode steps: 200, steps per second: 506, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.592, 0.615], loss: 9.323072, mean_absolute_error: 35.712448, mean_q: 71.065826\n",
      " 42270/50000: episode: 436, duration: 0.300s, episode steps: 153, steps per second: 510, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.294 [-1.996, 0.504], loss: 8.605943, mean_absolute_error: 35.473671, mean_q: 70.533073\n",
      " 42470/50000: episode: 437, duration: 0.451s, episode steps: 200, steps per second: 443, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.193 [-1.638, 0.433], loss: 7.129365, mean_absolute_error: 35.763481, mean_q: 71.067673\n",
      " 42655/50000: episode: 438, duration: 0.385s, episode steps: 185, steps per second: 481, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.246 [-1.969, 0.498], loss: 7.156325, mean_absolute_error: 35.577572, mean_q: 70.883469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42820/50000: episode: 439, duration: 0.326s, episode steps: 165, steps per second: 506, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.271 [-1.968, 0.505], loss: 7.374728, mean_absolute_error: 35.439671, mean_q: 70.594292\n",
      " 42984/50000: episode: 440, duration: 0.318s, episode steps: 164, steps per second: 515, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.279 [-1.843, 0.889], loss: 6.155617, mean_absolute_error: 35.511452, mean_q: 70.915909\n",
      " 43184/50000: episode: 441, duration: 0.389s, episode steps: 200, steps per second: 514, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.221 [-1.784, 0.742], loss: 9.353643, mean_absolute_error: 35.486382, mean_q: 70.720451\n",
      " 43384/50000: episode: 442, duration: 0.393s, episode steps: 200, steps per second: 508, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.200 [-1.861, 0.645], loss: 7.268468, mean_absolute_error: 35.523033, mean_q: 70.541412\n",
      " 43536/50000: episode: 443, duration: 0.301s, episode steps: 152, steps per second: 506, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.270 [-1.822, 0.390], loss: 8.758675, mean_absolute_error: 35.684242, mean_q: 70.821106\n",
      " 43727/50000: episode: 444, duration: 0.392s, episode steps: 191, steps per second: 488, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.242 [-1.972, 0.426], loss: 8.489176, mean_absolute_error: 35.360371, mean_q: 70.422997\n",
      " 43927/50000: episode: 445, duration: 0.391s, episode steps: 200, steps per second: 512, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.744, 0.574], loss: 4.354960, mean_absolute_error: 35.307846, mean_q: 70.204155\n",
      " 44063/50000: episode: 446, duration: 0.284s, episode steps: 136, steps per second: 480, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.235 [-1.416, 0.426], loss: 9.292629, mean_absolute_error: 35.166859, mean_q: 69.753754\n",
      " 44204/50000: episode: 447, duration: 0.282s, episode steps: 141, steps per second: 500, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.297 [-1.983, 0.686], loss: 7.153371, mean_absolute_error: 35.331047, mean_q: 70.314880\n",
      " 44384/50000: episode: 448, duration: 0.346s, episode steps: 180, steps per second: 520, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.255 [-1.851, 0.530], loss: 7.467367, mean_absolute_error: 35.434059, mean_q: 70.318977\n",
      " 44584/50000: episode: 449, duration: 0.383s, episode steps: 200, steps per second: 522, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.127 [-1.431, 0.571], loss: 4.011229, mean_absolute_error: 35.327824, mean_q: 70.329903\n",
      " 44784/50000: episode: 450, duration: 0.388s, episode steps: 200, steps per second: 516, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.148 [-1.276, 0.645], loss: 6.656143, mean_absolute_error: 35.412819, mean_q: 70.252617\n",
      " 44955/50000: episode: 451, duration: 0.427s, episode steps: 171, steps per second: 400, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.293 [-1.992, 0.574], loss: 6.855882, mean_absolute_error: 35.510338, mean_q: 70.705177\n",
      " 45110/50000: episode: 452, duration: 0.304s, episode steps: 155, steps per second: 509, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.291 [-2.029, 0.701], loss: 4.947809, mean_absolute_error: 35.713398, mean_q: 71.153320\n",
      " 45310/50000: episode: 453, duration: 0.566s, episode steps: 200, steps per second: 354, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-0.628, 0.511], loss: 7.559274, mean_absolute_error: 35.597721, mean_q: 70.926094\n",
      " 45510/50000: episode: 454, duration: 0.391s, episode steps: 200, steps per second: 511, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.122 [-1.159, 0.574], loss: 7.661223, mean_absolute_error: 35.326927, mean_q: 70.316536\n",
      " 45710/50000: episode: 455, duration: 0.472s, episode steps: 200, steps per second: 423, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.047 [-0.592, 0.583], loss: 9.384892, mean_absolute_error: 35.110294, mean_q: 69.694801\n",
      " 45910/50000: episode: 456, duration: 0.464s, episode steps: 200, steps per second: 431, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.060 [-0.605, 0.520], loss: 6.307262, mean_absolute_error: 35.680542, mean_q: 71.101395\n",
      " 46110/50000: episode: 457, duration: 0.397s, episode steps: 200, steps per second: 503, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.034 [-0.613, 0.602], loss: 6.732058, mean_absolute_error: 35.579262, mean_q: 70.841934\n",
      " 46310/50000: episode: 458, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.099 [-0.956, 0.522], loss: 3.921801, mean_absolute_error: 35.809166, mean_q: 71.314232\n",
      " 46470/50000: episode: 459, duration: 0.371s, episode steps: 160, steps per second: 431, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.282 [-1.846, 0.535], loss: 4.070406, mean_absolute_error: 35.883598, mean_q: 71.740517\n",
      " 46659/50000: episode: 460, duration: 0.409s, episode steps: 189, steps per second: 462, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.266 [-2.030, 0.770], loss: 7.128090, mean_absolute_error: 35.870907, mean_q: 71.261597\n",
      " 46796/50000: episode: 461, duration: 0.272s, episode steps: 137, steps per second: 504, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.337 [-2.025, 0.497], loss: 7.944354, mean_absolute_error: 35.451359, mean_q: 70.401901\n",
      " 46996/50000: episode: 462, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.160 [-0.489, 1.044], loss: 6.389740, mean_absolute_error: 35.728615, mean_q: 71.126183\n",
      " 47168/50000: episode: 463, duration: 0.335s, episode steps: 172, steps per second: 513, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.263 [-1.669, 0.437], loss: 7.250385, mean_absolute_error: 35.787487, mean_q: 71.113441\n",
      " 47368/50000: episode: 464, duration: 0.386s, episode steps: 200, steps per second: 518, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.199 [-1.787, 0.521], loss: 6.680866, mean_absolute_error: 35.719833, mean_q: 71.218224\n",
      " 47568/50000: episode: 465, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.604, 0.544], loss: 6.971394, mean_absolute_error: 35.824059, mean_q: 71.247864\n",
      " 47749/50000: episode: 466, duration: 0.351s, episode steps: 181, steps per second: 516, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.236 [-1.610, 0.431], loss: 6.724008, mean_absolute_error: 36.040707, mean_q: 71.924034\n",
      " 47949/50000: episode: 467, duration: 0.386s, episode steps: 200, steps per second: 518, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.181 [-1.600, 0.743], loss: 5.064866, mean_absolute_error: 35.878078, mean_q: 71.497200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48111/50000: episode: 468, duration: 0.312s, episode steps: 162, steps per second: 519, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.256 [-1.810, 0.508], loss: 6.443087, mean_absolute_error: 36.188934, mean_q: 71.995369\n",
      " 48311/50000: episode: 469, duration: 0.390s, episode steps: 200, steps per second: 513, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.036 [-0.585, 0.636], loss: 6.075263, mean_absolute_error: 35.692295, mean_q: 71.093468\n",
      " 48487/50000: episode: 470, duration: 0.447s, episode steps: 176, steps per second: 394, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.271 [-1.860, 0.519], loss: 6.815546, mean_absolute_error: 36.269474, mean_q: 72.201973\n",
      " 48687/50000: episode: 471, duration: 0.584s, episode steps: 200, steps per second: 342, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.192 [-0.556, 1.272], loss: 6.578265, mean_absolute_error: 35.970497, mean_q: 71.654884\n",
      " 48866/50000: episode: 472, duration: 0.348s, episode steps: 179, steps per second: 514, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.221 [-1.416, 0.396], loss: 7.001140, mean_absolute_error: 35.912540, mean_q: 71.490318\n",
      " 49060/50000: episode: 473, duration: 0.366s, episode steps: 194, steps per second: 530, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.235 [-1.819, 0.448], loss: 5.042866, mean_absolute_error: 35.953644, mean_q: 71.308159\n",
      " 49260/50000: episode: 474, duration: 0.390s, episode steps: 200, steps per second: 513, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.112 [-1.082, 0.601], loss: 7.758866, mean_absolute_error: 36.094532, mean_q: 71.454437\n",
      " 49440/50000: episode: 475, duration: 0.351s, episode steps: 180, steps per second: 513, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.253 [-1.858, 0.555], loss: 7.526631, mean_absolute_error: 36.173138, mean_q: 71.704613\n",
      " 49640/50000: episode: 476, duration: 0.392s, episode steps: 200, steps per second: 510, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.109 [-1.116, 0.437], loss: 7.132284, mean_absolute_error: 35.620041, mean_q: 70.470879\n",
      " 49840/50000: episode: 477, duration: 0.396s, episode steps: 200, steps per second: 505, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.599, 0.600], loss: 6.375785, mean_absolute_error: 35.944115, mean_q: 71.023239\n",
      " 49995/50000: episode: 478, duration: 0.315s, episode steps: 155, steps per second: 491, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.299 [-2.008, 0.581], loss: 5.553045, mean_absolute_error: 35.565365, mean_q: 70.465523\n",
      "done, took 156.844 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd5f1f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 168.000, steps: 168\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd5f1048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=500, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn.test(env, nb_episodes=50, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\lib\\site-packages\\gym\\__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 625\n",
      "Trainable params: 625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 3)            0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4)            0           action_input[0][0]               \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           160         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           1056        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 32)           1056        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            33          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 1)            0           dense_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,305\n",
      "Trainable params: 2,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: -6.5680\n",
      "50 episodes - episode_reward: -1313.609 [-1790.855, -1029.613] - loss: 2.368 - mean_absolute_error: 0.470 - mean_q: -31.107\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: -4.4307\n",
      "50 episodes - episode_reward: -886.136 [-1495.695, -482.151] - loss: 12.802 - mean_absolute_error: 1.053 - mean_q: -73.456\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: -1.8887\n",
      "50 episodes - episode_reward: -377.750 [-1495.803, -6.009] - loss: 19.375 - mean_absolute_error: 1.442 - mean_q: -85.669\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: -0.7566\n",
      "50 episodes - episode_reward: -151.315 [-413.706, -0.803] - loss: 21.069 - mean_absolute_error: 1.669 - mean_q: -79.213\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: -1.3280\n",
      "done, took 226.837 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "gym.undo_logger_setup()\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "assert len(env.action_space.shape) == 1\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=200)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-34742d5168e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_max_episode_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits\n",
    "# @misc{plappert2016kerasrl,\n",
    "#    author = {Matthias Plappert},\n",
    "#    title = {keras-rl},\n",
    "#    year = {2016},\n",
    "#   publisher = {GitHub},\n",
    "#   journal = {GitHub repository},\n",
    "#  howpublished = {\\url{https://github.com/keras-rl/keras-rl}},\n",
    "#}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
